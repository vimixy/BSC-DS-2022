{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web-Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Parse HTML elements in webpages\n",
    "- Use requests and BeautifulSoup to get and process webpage contents\n",
    "- Use ethics when scraping websites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, you can get all the data you need easily and accessibly, and don't need to scour the web to find a source that will let you do your analysis. \n",
    "\n",
    "We'd all prefer one of these:\n",
    "\n",
    "<img src=\"images/other_options.png\" alt=\"image showcasing a downloadable csv, database connection, or API, but we're not always so lucky. not sure of image source, took from materials provided by another instructor\" width=650>\n",
    "\n",
    "But we're not always so lucky! Sometimes we need data that's less accessible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enter...\n",
    "\n",
    "<img alt=\"beautiful soup logo\" src=\"images/bs.png\" width=500>\n",
    "\n",
    "> \"You didn't write that awful page. You're just trying to get some data out of it. Beautiful Soup is here to help. Since 2004, it's been saving programmers hours or days of work on quick-turnaround screen scraping projects.\"\n",
    "\n",
    "- From the Beautiful Soup [documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The components of a web page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "When we visit a web page, our web browser makes a GET request to a web server. The server then sends back files that tell our browser how to render the page for us. The files fall into a few main types:\n",
    "\n",
    "- HTML — contain the main content of the page.\n",
    "- CSS — add styling to make the page look nicer.\n",
    "- JS — Javascript files add interactivity to web pages.\n",
    "- Images — image formats, such as JPG and PNG allow web pages to show pictures.\n",
    "\n",
    "After our browser receives all the files, it renders the page and displays it to us. There’s a lot that happens behind the scenes to render a page nicely, but we don’t need to worry about most of it when we’re web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "HyperText Markup Language (HTML) is a language that web pages are created in. HTML isn’t a programming language, like Python — instead, it’s a markup language that tells a browser how to layout content. Let’s take a quick tour through HTML so we know enough to scrape effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "HTML consists of elements called tags.\n",
    "\n",
    "Tags have commonly used names that depend on their position in relation to other tags:\n",
    "\n",
    "- **child** — a child is a tag inside another tag. \n",
    "- **parent** — a parent is the tag another tag is inside. \n",
    "- **sibling** — a sibiling is a tag that is nested inside the same parent as another tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here's som example HTML - which tags are parents? children? siblings?\n",
    "\n",
    "~~~html\n",
    "<html>\n",
    "  <head></head>\n",
    "  <body>\n",
    "    <p>\n",
    "      Here's a paragraph of text!\n",
    "      <a href=\"https://www.dataquest.io\">Learn Data Science Online</a>\n",
    "    </p>\n",
    "    <p>\n",
    "      Here's a second paragraph of text!\n",
    "      <a href=\"https://www.python.org\">Python</a>        \n",
    "    </p>\n",
    "  </body>\n",
    "</html>\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grabbing Movie Data\n",
    "\n",
    "We might think about grabbing more movie data, as we gear up for our Phase 1 project which uses movie data. \n",
    "\n",
    "If we go to [IMDB](https://www.imdb.com/), their only API content seems expensive, and their advanced search results in tabular data that seems _extremely_ scrapable.\n",
    "\n",
    "**BUT** \n",
    "\n",
    "Enter - [conditions of use pages](https://www.imdb.com/conditions) ... and ethics!\n",
    "\n",
    "> \"**Robots and Screen Scraping:** You may not use data mining, robots, screen scraping, or similar data gathering and extraction tools on this site, except with our express written consent as noted below.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ethical Concerns\n",
    "\n",
    "- Terms of Service (includes the conditions of use shown above)\n",
    "- Denial of Service Attacks\n",
    "- Confidentiality\n",
    "\n",
    "[This article](https://oxylabs.io/blog/is-web-scraping-legal) discusses legal issues related to web scraping.\n",
    "\n",
    "Key points: \n",
    "\n",
    "- Don't log into a site and then scrape what's only available after logging in - then, you're likely violating the Terms of Service (you can always check to see if that's actually covered in the ToS)\n",
    "- Don't scrape copyrighted data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's Discuss**\n",
    "\n",
    "- Do people scrape sites they shouldn't? Sure, all the time. But am I going to tell you to ignore conditions/terms of use? Absolutely not. Make good choices.\n",
    "\n",
    "_We are not lawyers - this does not constitute legal advice._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let's scrape Wikipedia for movie data - Wikipedia has a very accessible Creative Commons license for use!\n",
    "\n",
    "Let's explore a few [years in film](https://en.wikipedia.org/wiki/Table_of_years_in_film).\n",
    "\n",
    "## Task: Grab the top 10 highest-grossing films for each year, 2000-2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "\n",
    "Our goal is to collect data into a Pandas dataframe. Plus we're still working with websites, so we'll still need the requests library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup # note this odd import statement structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we may also need lxml - https://lxml.de/index.html\n",
    "# helps process html or xml in python\n",
    "# !pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test case - [the year 2000](https://en.wikipedia.org/wiki/2000_in_film)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the response from the website, using requests\n",
    "resp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's check out the text attribute of that response...\n",
    "resp.text\n",
    "# (ew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now... beautiful soup! Let's soup-ify that text attribute\n",
    "soup = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Can use a prettify function to pretty print\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to find the table we want in the soup - use .find()\n",
    "# Can pass a dictionary in the attributes argument\n",
    "table = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Explore that result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the first real row in the table\n",
    "# Can use get_text as a method, then use string methods!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the last row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can make into a dataframe like we did with an API result\n",
    "# First need to define the columns\n",
    "columns = table.find_all('tr')[0].get_text().split('\\n')[1:5]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's make a list of rows for the data\n",
    "row_list = []\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    row_list.append(row.get_text().split('\\n\\n'))\n",
    "    \n",
    "row_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([pd.DataFrame([i], columns = columns) for i in row_list], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean that up... Using an applymap!\n",
    "df.applymap(lambda x: x.replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**But wait...** there's a shortcut (thanks pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check out the read_html method\n",
    "# Note - pandas likes the prettify objects better\n",
    "pd.read_html(table.prettify())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_html(table.prettify())[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can add a column saying which year this ranking is from\n",
    "df['Year'] = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Loop It!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My preference - create a list of dataframes, then concat afterwards\n",
    "# Are there other ways to create one big df from this? OF COURSE!\n",
    "\n",
    "list_of_dfs = []\n",
    "\n",
    "for year in range(2000, 2021):\n",
    "    url = f\"https://en.wikipedia.org/wiki/{year}_in_film\"\n",
    "    resp = requests.get(url).text\n",
    "    soup = BeautifulSoup(resp)\n",
    "    table = soup.find('table', {'class':\"wikitable sortable\"})\n",
    "    df = pd.read_html(table.prettify())[0]\n",
    "    df['Year'] = year\n",
    "    list_of_dfs.append(df)\n",
    "    # Only 20 things... not going to worry about using time to pause requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the last one\n",
    "list_of_dfs[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to concat...\n",
    "full_df = pd.concat([df for df in list_of_dfs], ignore_index=True)\n",
    "full_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice some data cleaning on the Worldwide Gross column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can see a hint of some gross data in the Gross column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check out that complicated example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out how to clean it...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's do it on the whole column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check our work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Can also remove commas and dollar signs, and make the column integers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity check\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion Time!\n",
    "\n",
    "What else could we do with webscraping? Any project ideas pop into mind? Any useful things on that page we could also use to grab more data? Let's discuss!\n",
    "\n",
    "- Had URLs in these results - could grab even more data on each movie using those\n",
    "- Can loop through any kind of repeatable URL, provided you figure out the pattern!\n",
    "- The possibilities are endless... (but don't forget to check the terms of use or copyright!)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
