{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Feature Engineering\n",
    "\n",
    "Featuring an exploration of polynomial and interaction terms (postponed from last Thursday!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Use correlations and other algorithms to inform feature selection\n",
    "- Address the problem of multicollinearity in regression problems\n",
    "- Create new features for use in modeling\n",
    "    - Use `PolynomialFeatures` to build compound features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "Insurance costs data (from https://www.kaggle.com/mirichoi0218/insurance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/insurance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly encode our categorical variables to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set our X and y\n",
    "X = df.drop(columns='charges')\n",
    "y = df['charges']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['sex', 'smoker', 'region']\n",
    "\n",
    "# create an encoder object\n",
    "encoder = OneHotEncoder(handle_unknown='error',\n",
    "                        drop='first',\n",
    "                        categories='auto')\n",
    "\n",
    "# Create an columntransformer object\n",
    "ct = ColumnTransformer(transformers=[('ohe', encoder, cat_cols)],\n",
    "                       remainder='passthrough')\n",
    "ct.fit(X_train)\n",
    "X_train_enc = ct.transform(X_train)\n",
    "X_test_enc = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also go ahead and scale - let's use a MinMaxScaler because binaries!\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# train on train data\n",
    "scaler.fit(X_train_enc)\n",
    "\n",
    "# transform both train and test data\n",
    "X_train_scaled = scaler.transform(X_train_enc)\n",
    "X_test_scaled = scaler.transform(X_test_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_sc_df = pd.DataFrame(X_train_scaled, columns= ct.get_feature_names())\n",
    "X_train_sc_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Correlation and Multicollinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Our first attempt might be just see which features are _correlated_ with the target to make a prediction.\n",
    "\n",
    "We can use the correlation metric in making a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to create a full train df with both X and y variables to explore\n",
    "train_df = pd.DataFrame(X_train_enc, columns= ct.get_feature_names())\n",
    "train_df['target'] = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makign the visual bigger so we can read it\n",
    "sns.set(rc={'figure.figsize':(8, 8)})\n",
    "\n",
    "sns.heatmap(train_df.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's zoom in on the correlations with 'charges' (target)\n",
    "train_df.corr()['target'].map(abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that none of these features are super strongly correlated with our target... so it's not surprising if a model of these features on this target doesn't perform particularly well!\n",
    "\n",
    "But, let's try to model, first with just the most correlated feature (`children`) and then with all features, and see how they perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Instantiate our simple model\n",
    "lr_simple = LinearRegression()\n",
    "\n",
    "# Run with our most strongly correlated feature\n",
    "lr_simple.fit(X_train_sc_df[['children']], y_train)\n",
    "\n",
    "# Score on train\n",
    "print(f\"Train R2: {lr_simple.score(X_train_sc_df[['children']], y_train):.4f}\")\n",
    "\n",
    "# Make a df version of test to score it too\n",
    "X_test_sc_df = pd.DataFrame(X_test_scaled, columns= ct.get_feature_names())\n",
    "print(f\"Test R2: {lr_simple.score(X_test_sc_df[['children']], y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "lr_all = LinearRegression()\n",
    "\n",
    "# Run with all features\n",
    "lr_all.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Score on train and test\n",
    "print(f\"Train R2: {lr_all.score(X_train_scaled, y_train):.4f}\")\n",
    "print(f\"Test R2: {lr_all.score(X_test_scaled, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Coefficients\n",
    "\n",
    "Our simple model with just one variable performed quite poorly, but our more complicated model performed much better. \n",
    "\n",
    "Let's explore the coefficients of that model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(ct.get_feature_names(), lr_all.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BECAUSE our data is all on the same scale, we can use coefficients to decide which are more important in this model!\n",
    "\n",
    "Let's run another model with only the top 4 features with the largest coefficients (by absolute value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our top four features for train and test\n",
    "top4 = None\n",
    "# Easiest to do this with a dataframe\n",
    "X_train_top4 = X_train_sc_df[top4]\n",
    "X_test_top4 = X_test_sc_df[top4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "lr_top4 = LinearRegression()\n",
    "\n",
    "# Run with all features\n",
    "lr_top4.fit(X_train_top4, y_train)\n",
    "\n",
    "# Score on train and test\n",
    "print(f\"Train R2: {lr_top4.score(X_train_top4, y_train):.4f}\")\n",
    "print(f\"Test R2: {lr_top4.score(X_test_top4, y_test):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Recursive Feature Elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The idea behind recursive feature elimination is to start with all predictive features and then build down to a small set of features slowly, by eliminating the features with the lowest coefficients.\n",
    "\n",
    "That is:\n",
    "\n",
    "1. Start with a model with _all_ $n$ predictors\n",
    "2. find the predictor with the smallest effect (coefficient)\n",
    "3. throw that predictor out and build a model with the remaining $n-1$ predictors\n",
    "4. set $n = n-1$ and repeat until $n-1$ has the value you want!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Recursive Feature Elimination in Scikit-Learn\n",
    "\n",
    "Note: MUST use on scaled data!\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RFE\n",
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr_rfe = LinearRegression()\n",
    "select = RFE(lr_rfe, n_features_to_select=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "select.fit(X=X_train_scaled, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "select.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "select.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are more options built out in SKLearn - check out their [user guide section on feature selection](https://scikit-learn.org/stable/modules/feature_selection.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Terms - Simple Linear Regression\n",
    "\n",
    "Demonstrating this on a toy example, with a single x variable predicting y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 samples from uniform distribution between -2pi and 2pi\n",
    "\n",
    "x = np.random.uniform(-2*np.pi, 2*np.pi, 150)\n",
    "\n",
    "# Creating target (y) - so we know the true relationship between x and y\n",
    "# But - adding some noise (error) with 'np.random'\n",
    "\n",
    "y = np.sin(x) + np.random.normal(loc=0, scale=0.4, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.ylabel('$\\sin(x)$ plus noise')\n",
    "plt.xlabel('x values are randomly chosen from $[-2\\pi, 2\\pi]$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear model\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the predicted values\n",
    "y_pred = lr.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring our model\n",
    "print(f\"R2 Score: {r2_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y) # original data\n",
    "\n",
    "plt.plot(x, y_pred, c='red') # predicted values\n",
    "\n",
    "plt.ylabel('$\\sin(x)$ + noise')\n",
    "plt.xlabel('x values randomly chosen between $-2\\pi$ and $2\\pi$')\n",
    "plt.title(\"Simple Linear Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good model? Well - of course not. It's definitely **underfit** - it is not complex enough to accurately capture the pattern and predict the target.\n",
    "\n",
    "Let's try again, but now with polynomials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this, we'll need some helper functions\n",
    "# Shoutout to Andy for sending me these\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def create_poly_dataset(x, degree):\n",
    "    \"\"\"\n",
    "    returning dataset with the given polynomial degree\n",
    "    \"\"\"\n",
    "    # Instantiate the PolynomialFeatures object with given 'degree'\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Now transform data to create higher order features\n",
    "    new_data = poly.fit_transform(x.reshape(-1, 1))\n",
    "    return new_data\n",
    "\n",
    "def fit_linear_model(data, y):\n",
    "    \"\"\"\n",
    "    fitting a linear model and printing model details\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    lr.fit(data, y)\n",
    "    print(\"-\"*13)\n",
    "    print(\"Coefficients: \", lr.coef_)\n",
    "    y_pred = lr.predict(data)\n",
    "    print(f\"R-Squared: {lr.score(data, y):.3f}\")\n",
    "    return lr\n",
    "\n",
    "def plot_predict(x, y, model):\n",
    "    \"\"\"\n",
    "    plotting predictions against true values\n",
    "    \"\"\"\n",
    "    plt.scatter(x, y, label='true')\n",
    "    x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "    \n",
    "    # visualize beyond this x range by uncommenting below:\n",
    "#     extra = x.ptp() * .2\n",
    "#     x_pred = np.linspace(x.min() - extra, x.max() + extra, 100)\n",
    "\n",
    "    plt.plot(x_pred, model.predict(create_poly_dataset(x_pred, len(model.coef_)-1)),\n",
    "             label='predicted', c='red')\n",
    "\n",
    "    if len(model.coef_) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms \\n (no slope)\")\n",
    "    elif (len(model.coef_) - 1) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Term\")\n",
    "    else:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing an assortment of polynomial degrees\n",
    "# can visualize each sequential polynomial with `range(n)`\n",
    "for i in [0, 1, 2, 3, 5, 7, 9, 13, 18]:\n",
    "    xi = create_poly_dataset(x, i)\n",
    "    plot_predict(x, y, fit_linear_model(xi, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: which of these is the best?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: so what?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When do we need interaction terms? And how do we check for them?\n",
    "\n",
    "Well, first things first - what interactions do _you_ think would make sense? That's the easiest way to incorporate interaction terms - use domain knowledge to think through what usefully could be combined into an interaction.\n",
    "\n",
    "As for how to check if something might be better captured as an interaction..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Quick set up\n",
    "df_ohe = pd.get_dummies(df, columns=cat_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of no interaction term...\n",
    "sns.lmplot(x='age', y='charges', hue='smoker_yes', data=df_ohe, scatter=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do I know these two variables, `age` and `smoker_yes`, aren't interacting? \n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's look at something else...\n",
    "sns.lmplot(x='bmi', y='charges', hue='smoker_yes', data=df_ohe, scatter=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Interaction and Polynomials in Sklearn\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's a sklearn function for both!\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first do Polynomials, to the 3rd degree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up our PolynomialFeatures with degree=3 and interaction_only=False\n",
    "poly = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = poly.transform(X_train_scaled)\n",
    "X_test_poly = poly.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_poly = pd.DataFrame(X_train_poly, columns = poly.get_feature_names())\n",
    "X_train_poly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_poly.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train_poly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model!\n",
    "lr_poly = LinearRegression()\n",
    "\n",
    "lr_poly.fit(X_train_poly, y_train)\n",
    "\n",
    "train_poly_preds = lr_poly.predict(X_train_poly)\n",
    "test_poly_preds = lr_poly.predict(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# evaluate\n",
    "print(f\"Train R2 Score: {r2_score(y_train, train_poly_preds):.3f}\")\n",
    "print(f\"Test R2 Score: {r2_score(y_test, test_poly_preds):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize residuals, for the model that now has many polynomial cols\n",
    "plt.scatter(train_poly_preds, y_train-train_poly_preds, label='Train')\n",
    "plt.scatter(test_poly_preds, y_test-test_poly_preds, label='Test')\n",
    "\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('residuals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's set up interactions: degree=2, interaction_only=True\n",
    "interactions = None\n",
    "\n",
    "interactions.fit(X_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ints = interactions.transform(X_train_scaled)\n",
    "X_test_ints = interactions.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ints = pd.DataFrame(X_train_ints, columns = interactions.get_feature_names())\n",
    "X_train_ints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model!\n",
    "lr_int = LinearRegression()\n",
    "\n",
    "lr_int.fit(X_train_ints, y_train)\n",
    "\n",
    "train_ints_preds = lr_int.predict(X_train_ints)\n",
    "test_ints_preds = lr_int.predict(X_test_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "print(f\"Train R2 Score: {r2_score(y_train, train_ints_preds):.3f}\")\n",
    "print(f\"Test R2 Score: {r2_score(y_test, test_ints_preds):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize residuals, for the model that now has interaction cols\n",
    "plt.scatter(train_ints_preds, y_train-train_ints_preds, label='Train')\n",
    "plt.scatter(test_ints_preds, y_test-test_ints_preds, label='Test')\n",
    "\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('residuals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate: What do you think? Is this blanket way of approaching polynomial or interaction terms useful?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources:\n",
    "\n",
    "[Feature Engineering and Selection: A Practical Approach for Predictive Models](https://bookdown.org/max/FES/) (computing done in R, but book focuses mostly on discussing the hows and whys rather than focusing on implementation)\n",
    "\n",
    "- And their chapter on [Detecting Interaction Effects](https://bookdown.org/max/FES/detecting-interaction-effects.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
