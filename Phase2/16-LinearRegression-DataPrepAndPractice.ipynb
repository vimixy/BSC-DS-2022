{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Practice - Plus Data Preprocessing/Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Problem\n",
    "\n",
    "We have used car data scraped from Belarus, in order to explore the used car market.\n",
    "\n",
    "Our goal is to build a model that effectively predicts the price of the used car based on its parameters (both numerical and categorical).\n",
    "\n",
    "## Data Understanding\n",
    "\n",
    "[Original Data Source](https://www.kaggle.com/datasets/lepchenkov/usedcarscatalog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/used_cars.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(), annot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=[object])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "### Deciding which columns to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the object columns, we can see that there are way too many `manufacturer_name`s and `model_name`s to One Hot Encode those. While those columns may be useful, they would require some extra work to get them in our model - let's just drop those columns for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Null Values\n",
    "\n",
    "One thing we haven't done so far this week is deal with many null values! However, if we try to throw our data into a model and it has null values, it will break the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out our null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss: How should we deal with these null values?\n",
    "\n",
    "-  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SKLearn Imputation User Guide: https://scikit-learn.org/stable/modules/impute.html#impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows where engine capacity is null\n",
    "# Easiest to do this before a train test split, honestly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While we could easily run the simple impute on the full dataset (no data leakage),\n",
    "# This seems like a good time for a train test split!\n",
    "\n",
    "X = None\n",
    "y = None\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can use simple imputer to fill nulls in color!\n",
    "# Import it here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate it with our strategy\n",
    "\n",
    "# Then fit and transform\n",
    "\n",
    "\n",
    "X_train_nonull = None\n",
    "X_test_nonull = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Our Categorical Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even though X_train_nonull is a np array, we can explore it a bit\n",
    "# Looping over each column to explore unique values\n",
    "for col in range(X_train_nonull.shape[1]):\n",
    "    col_uniques = np.unique(X_train_nonull[:,col])\n",
    "    print(len(col_uniques))\n",
    "    print(col_uniques)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see is that, for our four categorical columns (`transmission`, `color`, `engine_type`, and `body_type`), the maximum number of uniques is 13 - none of these have so many categories that we can't just one-hot encode these!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the same process we did yesterday\n",
    "# I went ahead and provided the imports\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our categorical columns - by index number, because np array\n",
    "cat_cols = [0, 1, 4, 6]\n",
    "\n",
    "# Instantiate our encoder\n",
    "encoder = OneHotEncoder(handle_unknown='error',\n",
    "                        drop='first',\n",
    "                        categories='auto')\n",
    "\n",
    "# Create an columntransformer object\n",
    "ct = ColumnTransformer(transformers=[('ohe', encoder, cat_cols)],\n",
    "                       remainder='passthrough', sparse_threshold=0)\n",
    "\n",
    "# Now fit and transform!\n",
    "\n",
    "\n",
    "X_train_enc = None\n",
    "X_test_enc = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can explore what this looks like now, at a glance\n",
    "pd.DataFrame(X_train_enc, columns=ct.get_feature_names()).info()\n",
    "# A note - newer versions of sklearn allow you to get better feature names\n",
    "# But we'll stick with the version of sklearn that's in IllumiDesk for now\n",
    "# Also I know these all appear to be objects - it's weird. They really aren't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Our Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scaler we want to use\n",
    "# Which should we use? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate, fit, then transform\n",
    "\n",
    "\n",
    "X_train_sc = None\n",
    "X_test_sc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-Less Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean of our training y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab predictions\n",
    "baseline_train_preds = None\n",
    "baseline_test_preds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(f\"Train R2 Score: {r2_score(y_train, baseline_train_preds):.4f}\")\n",
    "print(f\"Train MAE Score: ${mean_absolute_error(y_train, baseline_train_preds):.4f}\")\n",
    "print(f\"Train RMSE Score: ${mean_squared_error(y_train, baseline_train_preds, squared=False):.4f}\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Test R2 Score: {r2_score(y_test, baseline_test_preds):.4f}\")\n",
    "print(f\"Test MAE Score: ${mean_absolute_error(y_test, baseline_test_preds):.4f}\")\n",
    "print(f\"Test RMSE Score: ${mean_squared_error(y_test, baseline_test_preds, squared=False):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Thoughts?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the modeling library we want to use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit our model\n",
    "lr_base = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab predictions\n",
    "train_preds = None\n",
    "test_preds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(f\"Train R2 Score: {r2_score(y_train, train_preds):.4f}\")\n",
    "print(f\"Train MAE Score: ${mean_absolute_error(y_train, train_preds):.4f}\")\n",
    "print(f\"Train RMSE Score: ${mean_squared_error(y_train, train_preds, squared=False):.4f}\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Test R2 Score: {r2_score(y_test, test_preds):.4f}\")\n",
    "print(f\"Test MAE Score: ${mean_absolute_error(y_test, test_preds):.4f}\")\n",
    "print(f\"Test RMSE Score: ${mean_squared_error(y_test, test_preds, squared=False):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize residuals\n",
    "plt.scatter(train_preds, y_train-train_preds, label='Train')\n",
    "plt.scatter(test_preds, y_test-test_preds, label='Test')\n",
    "\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('residuals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Thoughts?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Model: Log Y\n",
    "\n",
    "We saw that our y value was pretty right-skewed to start with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Transforming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can work better if the predictor and target are normally distributed. \n",
    "\n",
    "**Log-transforming** can be a good tool to make *right-skewed* data more normal.\n",
    "\n",
    "(For *left-skewed* data, which is rarer, we can try transforming our data by raising it to an exponent greater than 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what transforming it would look like.\n",
    "\n",
    "Log transformation using numpy's `log1p` - [documentation](https://numpy.org/doc/stable/reference/generated/numpy.log1p.html) (Why `log1p`? Because math - [check out this post](https://stackoverflow.com/a/49538384/14222529). Also FYI inverse would be `expm1`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log1p(y_train).hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more normal! (although still some extreme outliers it looks like)\n",
    "\n",
    "Let's log both our train and test y, then see if our model improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = None\n",
    "y_test_log = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\n",
    "\n",
    "# Grab predictions\n",
    "train_preds_log = None\n",
    "test_preds_log = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "# Note that, for the two in dollar terms (MAE and RMSE), I unlog the predictions\n",
    "print(f\"Train R2 Score: {r2_score(y_train_log, train_preds_log):.4f}\")\n",
    "print(f\"Train MAE Score: ${mean_absolute_error(np.expm1(y_train_log), np.expm1(train_preds_log)):.4f}\")\n",
    "print(f\"Train RMSE Score: ${mean_squared_error(np.expm1(y_train_log), np.expm1(train_preds_log), squared=False):.4f}\")\n",
    "print(\"*\"*20)\n",
    "print(f\"Test R2 Score: {r2_score(y_test_log, test_preds_log):.4f}\")\n",
    "print(f\"Test MAE Score: ${mean_absolute_error(np.expm1(y_test_log), np.expm1(test_preds_log)):.4f}\")\n",
    "print(f\"Test RMSE Score: ${mean_squared_error(np.expm1(y_test_log), np.expm1(test_preds_log), squared=False):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize residuals\n",
    "plt.scatter(train_preds_log, y_train_log-train_preds_log, label='Train')\n",
    "plt.scatter(test_preds_log, y_test_log-test_preds_log, label='Test')\n",
    "\n",
    "plt.axhline(y=0, color = 'red', label = '0')\n",
    "plt.xlabel('predictions')\n",
    "plt.ylabel('residuals')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate: Thoughts?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting after Log Transformations\n",
    "\n",
    "But with this transformed target, how do I now interpret my LR coefficients?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at coefs for our ohe model\n",
    "dict(zip(ct.get_feature_names(), lr_base.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Now for coefs of our log model\n",
    "dict(zip(ct.get_feature_names(), lr_log.coef_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the transformation, I would have said that a one-unit increase (and note - units are based on however we scaled our inputs!) in the X column results on average in a `Xcoef` increase in our target. \n",
    "\n",
    "But what I need to say now is that a one-unit increase in our X column results on average in a `Xcoef` increase *in the logarithm of of our target*, i.e. an increase in price by a factor of $e^{\\text{Xcoef}}$.\n",
    "\n",
    "More practically, you can interpret the exponent as a percentage! If you take the exponent of the coefficient minus one, that gives you the percentage increase.\n",
    "\n",
    "Formula:\n",
    "\n",
    "$e ^ \\text{Xcoef} - 1$\n",
    "\n",
    "In code:\n",
    "```\n",
    "(np.exp(Xcoef) - 1) * 100\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For example:\n",
    "log_coef_dict = dict(zip(ct.get_feature_names(), lr_log.coef_))\n",
    "\n",
    "for feature, coef in log_coef_dict.items():\n",
    "    print(f\"A One-Unit Increase in {feature} results on average in a {(np.exp(coef) - 1) * 100:.4f}% change in price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our binary columns start to get really weird. In practice, before interpreting variables in any practical sense, we'd run one last model without scaling to allow us to better interpret our results - but we'll still likely keep our logged `y` as our target if it improves our model!\n",
    "\n",
    "Reference:\n",
    "- https://stats.oarc.ucla.edu/sas/faq/how-can-i-interpret-log-transformed-variables-in-terms-of-percent-change-in-linear-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next Model\n",
    "\n",
    "Now what?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here to keep iterating!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
