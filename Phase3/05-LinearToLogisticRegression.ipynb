{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear to Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Objectives\n",
    "\n",
    "- Describe conceptually the need to move beyond linear regression\n",
    "- Explain the form of logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification in Data Science: Predicting a Categorical Response\n",
    "\n",
    "![machine learning overview - now on to classification!](images/machinelearning-classification.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Classification techniques** are an essential part of machine learning and data mining applications. ***Most problems in Data Science are classification problems.***\n",
    "\n",
    "We will focus on *binary* classification problems for the rest of this phase, answering the question: \"Is it ___ or not?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Let's Explore\n",
    "\n",
    "Why do we need to introduce classification techniques?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "\n",
    "# Ignoring 'setting with copy' warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here we have a dataset about glass. Information [here](https://archive.ics.uci.edu/ml/datasets/glass+identification).\n",
    "\n",
    "Our goal: identify the type of glass - specifically, we'll create below a binary column called 'household type', which identifies the glass which is used in household items (as opposed to building windows or vehicles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# glass identification dataset\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/glass/glass.data'\n",
    "col_names = ['id','ri','na','mg','al','si','k','ca','ba','fe','household type']\n",
    "glass = pd.read_csv(url, names=col_names, index_col='id')\n",
    "glass['household type'] = np.where(glass['household type'] <=3, 0, 1) # cleaning our target\n",
    "\n",
    "# Just grab one predictor and our target\n",
    "# 'al' is the amount of aluminum content, household type is whether the glass is household glass\n",
    "df = glass[['al', 'household type']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's visualize the relationship between our predictor and our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df['al'], df['household type'])\n",
    "\n",
    "ax.set_yticks([0,1])\n",
    "ax.set_xlabel('level of aluminum content')\n",
    "ax.set_ylabel('whether the glass is household glass')\n",
    "ax.set_title('Type of Glass as a Function of Aluminum Content');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Using Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's draw a **regression line** to predict our target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fit a linear regression model and store the predictions as a column\n",
    "X = df[['al']]\n",
    "y = df['household type']\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X, y)\n",
    "\n",
    "df['preds'] = linreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# scatter plot that includes the regression line\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df['al'], df['household type'])\n",
    "ax.plot(df['al'], df['preds'], color='red', label='Predictions')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household glass type')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can see that this doesn't make a lot of sense. This straight line cannot grasp the true structure of what is going on when using a linear regression model. Now, without going into the mathematical details for now, let's look at a logistic regression model and fit that to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a logistic regression model and store the predictions as a column\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X, y)\n",
    "\n",
    "df['logreg_preds'] = logreg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "df = df.sort_values(by='al') # sorting so the plot renders correctly\n",
    "ax.scatter(df['al'], df['household type'])\n",
    "ax.plot(df['al'], df['logreg_preds'], color='red', label='Predictions')\n",
    "ax.set_xlabel('al')\n",
    "ax.set_ylabel('household')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Discuss the new shape:\n",
    "\n",
    "You can see that this function has an S-shape which plateaus to 0 in the left tail and 1 to the right tail. This is exactly what we needed here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That **S-shape** is what's known as a **sigmoid function**\n",
    "\n",
    "![sigmoid](images/SigmoidFunction_701.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might remember, a _linear_ regression model can be written as:\n",
    "\n",
    "$$ \\hat y = \\hat\\beta_0 + \\hat\\beta_1 x_1 + \\hat\\beta_2, x_2 +\\ldots + \\beta_n x_n $$\n",
    "\n",
    "When there are $n$ predictors $x_1,\\ldots,x_n$ and $n+1$ parameter estimates that are estimated by the model $\\hat\\beta_0, \\hat\\beta_1,\\ldots, \\hat\\beta_n$. $ \\hat y $ is an estimator for the outcome variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translating this model formulation to our example, this boils down to:\n",
    "\n",
    "$$ \\text{household glass} = \\beta_0 + \\beta_1 \\text{aluminum content} $$\n",
    "\n",
    "When you want to apply this to a binary target, what you actually want to do is perform a **classification** of your data in one group versus another one. In our case, we want to classify our observations as either \"household glass type\" or \"not household glass type\". A classification model will guess what the **probability** is of belonging to one group versus another. And that is exactly what logistic regression models can do! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation\n",
    "\n",
    "Essentially, what happens is, the linear regression is *transformed* in a way that the outcome takes a value between 0 and 1. This can then be interpreted as a probability (e.g., 0.2 is a probability of 20%). Applied to our example, the expression for a logistic regression model would look like this:\n",
    "\n",
    "$$ P(\\text{household glass}) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{aluminum content})}}$$\n",
    "\n",
    "Note that the outcome is written as $P(\\text{household glass})$. This means that the output should be interpreted as *the probability that glass is of the type 'household'*.\n",
    "\n",
    "In other words, the outcome variable should be interpreted as *the **probability** of the class label being equal to 1*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Specifics - with a side of more math\n",
    "\n",
    "#### What are the odds?\n",
    "\n",
    "<img src=\"images/odds.png\" width=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for an event with probability of 0.75, the odds of the event is:\n",
    "\n",
    "\n",
    "$$\\frac{0.75}{1 - 0.75} = \\frac{0.75}{0.25} = 3$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifically, we're modeling the LOG odds\n",
    "\n",
    "> Reminder: what are logs again? \n",
    "> $$ 2^4 = 16$$ $$\\log_2(16) = 4$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of odds range from 0 to infinity. When the value is less than 1, the odds are in favor of event not occuring; however, when odds is greater than, they are in favor of the events occuring. This poses a problem--the asymmetry. Therefore, we use the **log** of the odds to fix the asymmetry.\n",
    "\n",
    "Log(0.75/(1-0.75)) = 1.09\n",
    "\n",
    "Log(0.25/(1-0.25)) = -1.09"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### The Point\n",
    "\n",
    "Under the hood, logistic regression is a linear regression function, but the target is now the log odds of our binary outcome category. The result that we get out from logistic regression is a number between 0 and 1, a number I can naturally interpret as **the probability of our 1 class** (as in, probability that the output is 1). If I truly want a binary prediction, I can simply round the probability to then output either 0 or 1 (which sklearn's logistic regression function does for us!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to Glass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glass.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go beyond a simple logistic regression, and use all of these variables to predict household type!\n",
    "\n",
    "BUT we need to scale first! If we check out the documentation, we'll see that SKLearn's Logistic Regression function applies regularization by default - meaning we need to scale our inputs or else we'll unfairly penalize some coefficients!\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, train test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our scaler\n",
    "\n",
    "\n",
    "# Fit our scaler\n",
    "\n",
    "\n",
    "# Transform train and test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, time to model!\n",
    "\n",
    "# Instantiate our Logistic Regression model\n",
    "# We can explore and discuss the documentation - should we adjust any arguments?\n",
    "\n",
    "\n",
    "# Fit our model to the scaled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the default scorer to explore results!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's grab the predictions out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out predict_proba:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About `predict_proba`:\n",
    "\n",
    "[Documentation Link](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba)\n",
    "\n",
    "Basically: we have a column for each class in our target, and the number is the calculated probability estimate of each class for that row! Typically, we want to explore the probability of the `1` class (the target we're searching for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a dataframe to explore our predictions\n",
    "\n",
    "tr_outcomes = pd.DataFrame(y_train).rename(columns={'household type':'Actual Value'})\n",
    "\n",
    "tr_outcomes['Predicted Value'] = logreg.predict(X_train_scaled)\n",
    "tr_outcomes['Predicted Probability of 1'] = logreg.predict_proba(X_train_scaled)[:,1]\n",
    "\n",
    "tr_outcomes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretting coefficients\n",
    "\n",
    "This result, in combination with mathematical properties of exponential functions, leads to the fact that, applied to our example:\n",
    "\n",
    "> if *aluminum content* goes up by 1, the odds are multiplied by $e^{\\beta_1}$\n",
    "\n",
    "In our example, if there is a positive relationship between aluminum content and glass type, this will lead a positive $\\beta_1 > 0$, so $e^{\\beta_1}>1$, and the odds will increase as *aluminum content* increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the coefficients from our model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip them: need to access the 0th element of the coef_ argument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpret:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions ... should feel familiar\n",
    "\n",
    "- The independent variables should be linearly related to the log odds\n",
    "    - AKA we expect a linear relationship\n",
    "- The input variables should be independent of each other\n",
    "    - AKA the model should have little or no multicollinearity\n",
    "- For a binary logistic regression, the factor level 1 of the dependent variable should represent the desired outcome\n",
    "    - AKA the `1` class in the target column should always represent the thing you're trying to find\n",
    "- For a binary logistic regression, the dependent variable should be dichotomous in nature\n",
    "    - AKA the target column should be mutually exclusive\n",
    "    \n",
    "[Reference](https://www.statisticssolutions.com/free-resources/directory-of-statistical-analyses/what-is-logistic-regression/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "----\n",
    "\n",
    "## Level Up:  Deeper Dive:\n",
    "\n",
    "As mentioned before, the probability of identifying household glass can be calculated using:\n",
    "\n",
    "$$ P(\\text{household glass}) = \\displaystyle \\frac{1}{1+e^{-(\\hat \\beta_0+\\hat \\beta_1 \\text{aluminum content})}}$$\n",
    "\n",
    "You can show that, by multiplying both numerator and denominator by $e^{(\\hat \\beta_0+\\hat \\beta_1 \\text{aluminum content})}$\n",
    "\n",
    "\n",
    "$$ P(\\text{household glass}) = \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{aluminum content}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{aluminum content}}}$$\n",
    "\n",
    "As a result, you can compute $P(\\text{NOT household glass})$ as:\n",
    "\n",
    "$$ P(\\text{NOT household glass} ) = 1- \\displaystyle \\frac{e^{\\hat \\beta_0+\\hat \\beta_1 \\text{aluminum content}}}{1+e^{\\hat \\beta_o+\\hat \\beta_1 \\text{aluminum content}}}= \\displaystyle \\frac{1}{1+e^{\\hat \\beta_0+\\hat \\beta_1 \\text{aluminum content}}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Odds ratio\n",
    "\n",
    "This doesn't seem to be very spectacular, but combining these two results leads to an easy interpretation of the model parameters, triggered by the *odds*\n",
    "\n",
    "$$ \\dfrac{P(\\text{household glass})}{P(\\text{NOT household glass})} = e^{\\hat \\beta_0+\\hat \\beta_1 \\text{aluminum}} $$\n",
    "\n",
    "This expression can be interpreted as the *odds in favor of identifying household glass*."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
