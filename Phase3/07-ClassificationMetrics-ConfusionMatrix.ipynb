{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics: Introducing the Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Calculate and interpret a confusion matrix\n",
    "- Calculate and interpret classification metrics such as accuracy, recall, and precision\n",
    "- Choose classification metrics appropriate to a business problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are many ways to evaluate a classification model, and your choice of evaluation metric can have a major impact on how well your model serves its intended goals. This lecture will review common classification metrics you might consider using, and considerations for how to make your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start off with a page from [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/classification/true-false-positive-negative) and talk about a classic classification problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Boy Who Cried 'Wolf'\n",
    "\n",
    "![adorable wolf image from instagram user fablefire: https://www.instagram.com/p/CCGgVLGFneE/](images/awoo.png)\n",
    "\n",
    "In the old fable about the boy who cried 'wolf' there are two possible outcomes: \n",
    "\n",
    "- **No Wolf** - negative outcome, or 0\n",
    "- **Wolf** - positive outcome, or 1\n",
    "\n",
    "(I know, having a wolf arrive is not \"positive\" - but it is what we're trying to predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you think of this as a model, where the shepherd is predicting whether or not a wolf will threaten the flock of sheep:\n",
    "\n",
    "![outcome description for wolf scenarios as a confusion matrix](images/wolf_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what does that look like with data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All of the imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_openml, load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data from sklearn\n",
    "dfX, dfy = fetch_openml(\"titanic\", version=1, as_frame=True, return_X_y=True)\n",
    "# Cleaning a bit to get to a full dataframe of the data\n",
    "df = dfX.copy()\n",
    "df = df.drop(columns=['boat', 'body', 'home.dest'])\n",
    "df['survived'] = dfy\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['survived'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model-less Baseline\n",
    "\n",
    "First of all, I want to see how well the model will do if it predicts the majority class. In other words, if the model only predicts that no one survives, what percentage of the time would it be right? \n",
    "\n",
    "How do we do this? Find the number of passengers who didn't survive, divide by the total number of passengers - which `value_counts` will do for us if we set `normalize=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To visualize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actual = df['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_baseline = ['0'] * len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_actual, y_pred_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A confusion matrix\n",
    "confusion_matrix(y_actual, y_pred_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, prettier: \n",
    "\n",
    "<img alt=\"table view with colors to show results of modelless baseline\" src=\"images/full_titanic_modelless_baseline_cm.png\" height=200 width=200>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate:\n",
    "\n",
    "What is this showing us? Why two zeros on the right side?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix &rarr; Classification Metrics\n",
    "\n",
    "That block above, where we hashed out true negatives / true positives / false negatives / false positives, is called a **Confusion Matrix** - a summary of how well a classification model was able to predict each class. Across one axis you have the _predicted_ labels, and across the other axis you have the _actual_ labels, and thus you're able to clearly see the breakdown of where a model is making mistakes - and, more importantly, what kinds of mistakes your model is making.\n",
    "\n",
    "So - how does a confusion matrix translate into classification metrics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Interpretation\n",
    "\n",
    "\n",
    "<img alt=\"confusion matrix interpretation with metrics\" src=\"images/confusion_matrix_interpretation.png\" height=600 width=600>\n",
    "\n",
    "Note that I've highlighted the most often used metrics in blue above. \n",
    "\n",
    "In other words, those metrics are:\n",
    "\n",
    "- Accuracy: All True Predictions / All Predictions\n",
    "\n",
    "- Precision score: TP / All Predicted Positives\n",
    "\n",
    "- Recall or Sensitivity: TP / All Actual Positives \n",
    "\n",
    "There's one more score that's often referenced which balances precision and recall - it's called an [**F1 Score**](https://en.wikipedia.org/wiki/F1_score).\n",
    "\n",
    "$$ \\text{F1 Score} = 2 * \\frac{ precision * recall}{precision + recall} $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Let's Discuss**: Why might we care more about precision than recall, or vice versa? In other words, which one of these would you think is the **primary metric** for the business problem of predicting whether or not someone survived the Titanic?\n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's calculate the above highlighted classification metrics and consider which would be most useful for this scenario.\n",
    "\n",
    "First, though, we'll create a real model for the Titanic, generally using the strategy outlined by SKLearn [here](https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html) (although, we'll use just three columns, and we'll set `drop='first'` in our one hot encoder to reduce multicollinearity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "X = df[['pclass', 'sex', 'age']]\n",
    "y = df['survived']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our preprocessor\n",
    "numeric_features = [\"age\"]\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), \n",
    "           (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_features = [\"sex\", \"pclass\"]\n",
    "categorical_transformer = OneHotEncoder(handle_unknown=\"error\", drop='first')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fit our preprocessor, then transform train and test\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "X_train_pr = preprocessor.transform(X_train)\n",
    "X_test_pr = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit our model, then grab train and test predictions\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_pr, y_train)\n",
    "\n",
    "train_preds = model.predict(X_train_pr)\n",
    "test_preds = model.predict(X_test_pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confusion matrix for our test set\n",
    "cm = confusion_matrix(y_test, test_preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize that a bit nicer, using sklearn's function to plot CMs\n",
    "plot_confusion_matrix(model, X_test_pr, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate:\n",
    "\n",
    "What is a false positive in this context?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "What is a false negative in this context?\n",
    "\n",
    "- \n",
    "\n",
    "\n",
    "Which is worse?\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore Our Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our true positives, true negatives, false positives, and false negatives\n",
    "tn = cm[0, 0]\n",
    "fp = cm[0, 1]\n",
    "fn = cm[1, 0]\n",
    "tp = cm[1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Accuracy\n",
    "$\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "In words: How often did my model correctly identify whether or not someone survived? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is the default metric for most classification models, and thus is the score we get when we use `.score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test_pr, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Recall\n",
    "\n",
    "AKA **Sensitivity**\n",
    "\n",
    "$\\frac{TP}{TP + FN}$\n",
    "\n",
    "In words: How many of those who actually survived did my model identify? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Precision\n",
    "\n",
    "$\\frac{TP}{TP + FP}$\n",
    "\n",
    "In words: How often was my model's prediction of 'survived' correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### F-Scores\n",
    "\n",
    "An $F$-score is a combination of precision and recall, which can be useful when both are important for a business problem. \n",
    "\n",
    "Most common is the **$F_1$ Score**, which is an equal balance of the two using a [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean).\n",
    "\n",
    "$$F_1 = 2 \\frac{Pr \\cdot Rc}{Pr + Rc} = \\frac{2TP}{2TP + FP + FN}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Code it here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can generalize this score to the **$F_\\beta$ Score** where increasing $\\beta$ puts more importance on _recall_:\n",
    "\n",
    "$$F_\\beta =  \\frac{(1+\\beta^2) \\cdot Precision \\cdot Recall}{\\beta^2 \\cdot Precision + Recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## `classification_report()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can get all of these metrics using the `classification_report()` function. \n",
    "\n",
    "- The top rows (here, for 0 and 1) show statistics for if you treated each label as the \"positive\" class\n",
    "    - The scores we calculated above all match what is in the `1` row - that's our positive class\n",
    "- **Support** shows the sample size in each class\n",
    "- The averages in the bottom two rows are across the rows in the class table above (useful when there are more than two classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily SKLearn will of course calculate these scores for us. You can see all of their classification metrics [here](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exercise: Breast Cancer Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's evaulate a model using Scikit-Learn's breast cancer dataset. [Data description available here](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)\n",
    "\n",
    "This dataset has columns describing tumor details, and is predicting whether or not a tumor is benign. In our target column:\n",
    "- 0: Malignant\n",
    "- 1: Benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "preds, target = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    preds, target, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "\n",
    "# Run the model\n",
    "bc_model = LogisticRegression(solver='lbfgs', max_iter=10000, random_state=42)\n",
    "bc_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Calculate the following for this model:\n",
    "\n",
    "- Confusion Matrix\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here - confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Step 2:** Describe your business context:\n",
    "\n",
    "- What is a false positive in this context?\n",
    "\n",
    "    - \n",
    "    \n",
    "- What is a false negative in this context?\n",
    "\n",
    "    - \n",
    "    \n",
    "- Which is worse?\n",
    "\n",
    "    - \n",
    "    \n",
    "- Based on the above questions, which metric would you want to optimize on?\n",
    "\n",
    "    - \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
