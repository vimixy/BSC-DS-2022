{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Metrics: Utilizing Predicted Probabilities\n",
    "\n",
    "## Thresholding and the Receiver Operator Characteristic Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Calculate and interpret probability estimates\n",
    "- Adjust the threshold of a logistic regression model\n",
    "- Visualize, calculate and interpret the AUC-ROC metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now that we've learned how to evaluate a classification model's predictions, let's dig deeper to see how else we might evaluate our models and how we can use that information to improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario: Predicting Polymers\n",
    "\n",
    "<img src=\"https://images.pexels.com/photos/248152/pexels-photo-248152.jpeg?auto=compress&cs=tinysrgb&dpr=2&h=750&w=1260\" style='width:300px'>\n",
    "\n",
    "The researchers at SABIC Innovative Plastics US are working on refining the manufacturing process for new polymers. Creating this polymer involves combining 12 materials and 45 mechanical processes (order of processes is unimportant). The prices of ingredients vary as do the amount of time needed for each process done by individual machines that would otherwise be used to make other products. The CSuite requires a pared down list of materials and processes before polymers can go to market. \n",
    "\n",
    "A polymer yield greater than or equal to 41 grams is considered \"high\".\n",
    "\n",
    "You will use logistic regression to create a classification model find the material and process combination that produces high yield."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Problem\n",
    "\n",
    "#### End Users:\n",
    "\n",
    "- Scientists, budgeting department, CSuite\n",
    "\n",
    "#### True business problem:\n",
    "\n",
    "- Create a model that can, if given data from from repeated chemical manufacturing trials, predict if the polymer yield will be high (worth the money). \n",
    "\n",
    "#### Context: FILL IN THE BLANKS!\n",
    "\n",
    "- **False negative** in this context: ___\n",
    "    - **Outcome**: ___\n",
    "    \n",
    "- **False positive** in this context: ___\n",
    "\n",
    "    - **Outcome**: ___\n",
    "\n",
    "#### Evaluation \n",
    "\n",
    "Which metric (of the ones we've explore so far) would make sense to primarily use as we evaluate our models?\n",
    "\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall\n",
    "- F1-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, log_loss\n",
    "from sklearn.metrics import plot_confusion_matrix, plot_precision_recall_curve, plot_roc_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data & take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/ChemicalManufacturingProcess.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split\n",
    "\n",
    "Encode `Yield` into `1/0` depending on if it's equal or greater than 41."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "\n",
    "X = df.drop(columns = ['Yield'])\n",
    "y = (df['Yield'] >= 41).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- impute missing values in columns\n",
    "- scale using max absolute value (this was chosen after trial and error with other scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values with median\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale using MaxAbsScaler\n",
    "\n",
    "\n",
    "X_train_pr = None\n",
    "X_test_pr = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create logistic regression model\n",
    "\n",
    "Using all input features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state = 100)\n",
    "logreg.fit(X_train_pr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab train and test preds\n",
    "train_preds = logreg.predict(X_train_pr)\n",
    "test_preds = logreg.predict(X_test_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "Check the accuracy of the model. <br>\n",
    "Remember, `sklearn` uses a threshold cutoff of `0.5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Accuracy: {accuracy_score(y_train, train_preds)}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But is that the **BEST** cutoff?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "If you remember how the logistic regression model works, though, it doesn't actually generate predicted values of 0 or 1. It creates an S-shaped curve to approximate the data, estimating the _probability_ that they belong to the target class. This probability takes a value between 0 and 1. THEN sklearn will round up - anything above .5 becomes a 1, anything below .5 becomes a 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "source": [
    "<img src=\"https://www.graphpad.com/guides/prism/latest/curve-fitting/images/hmfile_hash_38a8acae.png\" width=600>\n",
    "\n",
    "Source: [GraphPad](https://www.graphpad.com/guides/prism/latest/curve-fitting/reg_simple_logistic_and_linear_difference.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Are We Stuck with the 0.5 Threshold?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "But we don't have to do things this way! Suppose we're building a model that predicts the presence of cancer from X-ray scans. And suppose we get a predicted probability of cancer for some particular scan that look like this:\n",
    "\n",
    "- pred_pos: 0.48\n",
    "\n",
    "Because false negatives (cancers not flagged) are *much* more costly than false positives (non-cancers flagged as cancers), we may well want to **adjust our threshold**. We might want to have our model predict \"positive\" if the corresponding probability is, say, 0.4, or maybe even as low as 0.1. (Speaking for myself, if there was even a 10% chance that I had cancer, I'd probably want to know about it.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can get these estimated probabilities using the `.predict_proba()` method. Each element gives two probabilities: the estimate probability of being in the 0 class (not high yield) and the 1 class (high yield)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted probabilities (log odds) from model for train and test set\n",
    "# slicing with [:,1] gets the 1st column, probabilities for the 1 class (high yield)\n",
    "train_probs = None\n",
    "test_probs = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Our Train Set's Predicted Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_compare = pd.DataFrame(y_train).rename(columns={'Yield':'Actual'})\n",
    "y_train_compare['Predicted'] = train_preds\n",
    "y_train_compare['Predicted Probabilities'] = train_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_compare.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the range of predicted probabilities using .describe\n",
    "y_train_compare[['Predicted Probabilities']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore where the model gets it wrong, and that range\n",
    "wrong_preds = y_train_compare.loc[y_train_compare['Actual'] != y_train_compare['Predicted']]\n",
    "wrong_preds[['Predicted Probabilities']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `for` loop to iterate over various thresholds and calculate metrics for both train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['threshold', 'mean_train_pred', 'mean_test_pred', 'train_acc', 'test_acc', \n",
    "             'train_prec', 'test_prec', 'train_rec', 'test_rec', 'train_f1', 'test_f1']\n",
    "metrics_df = pd.DataFrame(columns=col_names)\n",
    "\n",
    "for x in np.arange(train_probs.min(), train_probs.max(), 0.01):\n",
    "    threshold = x\n",
    "    \n",
    "    y_pred_train = np.where(train_probs > x, 1, 0)\n",
    "    y_pred_test = np.where(test_probs > x, 1, 0)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_pred_train)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    train_prec = precision_score(y_train, y_pred_train)\n",
    "    test_prec = precision_score(y_test, y_pred_test)\n",
    "    \n",
    "    train_rec = recall_score(y_train, y_pred_train)\n",
    "    test_rec = recall_score(y_test, y_pred_test)\n",
    "    \n",
    "    train_f1 = f1_score(y_train, y_pred_train)\n",
    "    test_f1 = f1_score(y_test, y_pred_test)\n",
    "    \n",
    "    placeholder = [threshold, y_pred_train.mean(), y_pred_test.mean(),\n",
    "                   train_acc, test_acc, train_prec, test_prec,\n",
    "                   train_rec, test_rec, train_f1, test_f1]\n",
    "    \n",
    "    sample_S = pd.Series(placeholder, index=col_names)\n",
    "    metrics_df = metrics_df.append(sample_S, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plot** the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 10))\n",
    "ax.set_title(\"Search for Best Threshold\", fontsize=16)\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.grid(True)\n",
    "\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# Get the regular numpy array from the MaskedArray\n",
    "X_axis = metrics_df['threshold'].values\n",
    "\n",
    "# Currently have all the train metrics hashed out - can unhash to visualize\n",
    "\n",
    "# ax.plot(X_axis, metrics_df['train_acc'], ls='--', color='g', label='Train Accuracy')\n",
    "ax.plot(X_axis, metrics_df['test_acc'], color='g', label='Test Accuracy')\n",
    "\n",
    "# ax.plot(X_axis, metrics_df['train_prec'], ls='--', color='b', label='Train Precision')\n",
    "ax.plot(X_axis, metrics_df['test_prec'], color='b', label='Test Precision')\n",
    "\n",
    "# ax.plot(X_axis, metrics_df['train_rec'], ls='--', color='r', label='Train Recall')\n",
    "ax.plot(X_axis, metrics_df['test_rec'], ls='-', color='r', label='Test Recall')\n",
    "\n",
    "# ax.plot(X_axis, metrics_df['train_f1'], ls='--', color='purple', label='Train F1')\n",
    "ax.plot(X_axis, metrics_df['test_f1'], ls='-', color='purple', label='Test F1')\n",
    "\n",
    "ax.vlines(.5, 0, 1, color='gray', ls='dotted', label='Threshold at .5')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question:\n",
    "\n",
    "Is `.5` the best cutoff?\n",
    "\n",
    "- \n",
    "  \n",
    "  \n",
    "What is the best choice?\n",
    "\n",
    "- \n",
    "  \n",
    "  \n",
    "Why?\n",
    "\n",
    "- \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust The Threshold\n",
    "\n",
    "If we found a better threshold, we need to implement that manually using the predicted probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can find exactly where to set our threshold using the metrics_df\n",
    "# Find the max value for the test metric we care about the most\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use np.where to create an output of 1s and 0s\n",
    "adj_train_preds = None\n",
    "adj_test_preds = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate:\n",
    "print(f\"Old Train F1: {f1_score(y_train, train_preds)}\")\n",
    "print(f\"Old Test F1: {f1_score(y_test, test_preds)}\")\n",
    "print(\"*\"*20)\n",
    "print(f\"New Train F1: {f1_score(y_train, adj_train_preds)}\")\n",
    "print(f\"New Test F1: {f1_score(y_test, adj_test_preds)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Further Exploring Probabilities\n",
    "\n",
    "### True Positive Rate\n",
    "\n",
    "True Positive Rate (TPR) is the same as *recall*, measuring how many of the positive cases we correctly classified as positive.\n",
    "\n",
    "> **True Positive Rate (TPR)** = **Recall** = $\\frac{TP}{TP + FN}$\n",
    "\n",
    "AKA: How many of the polymers with high yields did my model identify?\n",
    "\n",
    "\n",
    "### False Positive Rate\n",
    "\n",
    "False Positive Rate (FPR) measures how many of the negative casses we incorrectly classified as positive.\n",
    "\n",
    "> **False Positive Rate (TPR)** = $\\frac{FP}{FP + TN}$\n",
    "\n",
    "AKA: How many of the polymers without high yields did my model flag as high yield?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# The Receiver Operating Characteristic (ROC) Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The Receiver Operating Characteristic (ROC) curve plots the true-positive rate vs. the false-positive rate. Let's define these now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def classify_rates(y_test, y_probs, thresh):\n",
    "    y_pred = []\n",
    "    for val in y_probs:            # The val in y_probs shows the probability of 1\n",
    "        if val > thresh:                # We'll set our own threshold for classifying\n",
    "            y_pred.append(1)            # a test point as positive! The lower my threshold,\n",
    "        else:                           # the more predicted positives I'll have\n",
    "            y_pred.append(0)  \n",
    "    # Use a confusion matrix to get our tp/tn/fp/fn\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tp, tn, fp, fn = cm[1][1], cm[0][0], cm[0][1], cm[1][0]\n",
    "    # Calculate our true positive rate and false positive rate\n",
    "    tpr = tp / (tp + fn)\n",
    "    fpr = fp / (fp + tn)\n",
    "    return tpr, fpr, f'tpr:{round(tpr, 3)}, fpr:{round(fpr, 3)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "True- and false-positive rates for various thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x in np.linspace(0, 1, 11):\n",
    "    print(f'Rates at threshold = {x:.2f}: {classify_rates(y_test, test_probs, x)[2]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As my threshold goes up, I'll have fewer positive predictions, which means I'll have both fewer true positives and fewer false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Plotting the Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's plot our own ROC curve. We'll create an array of different thresholds and use our `classify_rates()` function to get the true- and false-positive rates for each threshold.\n",
    "\n",
    "One way of choosing a threshold **independently of business concerns** is to select the point on the curve that is furthest from (1, 0), the \"worse-case\" point where our true-positive rate is 0 and our false-positive rate is 1. So let's find that point as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tprs = []\n",
    "fprs = []\n",
    "diffs = []\n",
    "for x in np.linspace(0, 1, 101):\n",
    "    fprs.append(classify_rates(y_test, test_probs, x)[1])\n",
    "    tprs.append(classify_rates(y_test, test_probs, x)[0])\n",
    "    diffs.append(np.sqrt(tprs[-1]**2 + (1-fprs[-1])**2))\n",
    "    \n",
    "max_dist = diffs.index(np.max(diffs))\n",
    "print(f\"With a threshold of {(max_dist - 1) / 100}: \\n\"\n",
    "      f\"\\tYou\\'ll have a True Positive Rate of {tprs[max_dist]:.3f} \\n\"\n",
    "      f\"\\tand a False Positive Rate of {fprs[max_dist]:.3f}\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.plot(fprs[:max_dist], tprs[:max_dist], 'r.')\n",
    "ax.plot(fprs[max_dist], tprs[max_dist], 'ko', ms=10)\n",
    "ax.plot(fprs[max_dist + 1:], tprs[max_dist + 1:], 'r.')\n",
    "ax.plot(fprs, fprs, '.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### `plot_roc_curve()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can also use sklearn's `plot_roc_curve()` function with your fitted model and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plot_roc_curve(logreg, X_test_pr, y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Area Under the Curve (AUC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The ROC curve will be a plot of tpr (on the y-axis) vs. fpr (on the x-axis). There will always be a point at (0, 0) and another at (1, 1). The question is what happens in the middle. Since we want our y-values to be as high as possible for any particular x-value, a natural metric we can evaluate would be to calculate the **area under the curve**. \n",
    "\n",
    "> #### The larger the area, the better the classifier. \n",
    "\n",
    "The maximum possible area is the area of the whole box between 0 and 1 on both axes, so that's a **maximum area of 1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we have equal numbers of positives and negatives, then we can set an **absolute minimum area of 0.5**. That's the \"curve\" we'd get by plotting a straight diagonal line from (0, 0) to (1, 1).\n",
    "\n",
    "Why? The area under the curve really represents the test's ability to **discriminate** positives from negatives. Suppose I randomly took several pairs of points, one positive and one negative, and checked my test's predictions. The area under the receiver operator characteristic curve represents a **threshold-independent measure** of how often my test would get the two predictions correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Resources that Explain ROC-AUC Best:\n",
    "\n",
    "From Analytics Vidhya:\n",
    "\n",
    "> \"The Receiver Operator Characteristic (ROC) curve is an evaluation metric for binary classification problems. It is a probability curve that plots the TPR against FPR at various threshold values and essentially separates the ‘signal’ from the ‘noise’. The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve.\"\n",
    "\n",
    "| Perfect ROC Curve | More Typical ROC Curve | Bad ROC Curve (no better than guessing) |\n",
    "|---|---|---|\n",
    "|![perfect ROC curve](images/perfectAUC.webp) | ![more 'normal' ROC curve](images/midAUC.webp) | ![bad ROC curve](images/badAUC.webp) |\n",
    "\n",
    "---\n",
    "\n",
    "From [Google's Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc):\n",
    "\n",
    "> \"AUC provides an aggregate measure of performance across all possible classification thresholds. One way of interpreting AUC is **as the probability that the model ranks a random positive example more highly than a random negative example**. For example, given the following examples, which are arranged from left to right in ascending order of logistic regression predictions:\n",
    "> \n",
    "> ![auc predictions ranked](images/AUCPredictionsRanked.svg)\n",
    ">\n",
    "> \"AUC represents the probability that a random positive (green) example is positioned to the right of a random negative (red) example.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### AUC Calculation with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Scikit-Learn's `roc_auc_score()` function will compute the area under the curve for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Sidebar: Visualizing Threshold Changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This [ROC Applet](https://web.archive.org/web/20210210014824/http://www.navan.name/roc/) helps  visualize how a change in the threshold corresponds to moving along the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Scenario: Breast Cancer Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's evaluate a model using Scikit-Learn's breast cancer dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "preds, target = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(preds, target,\n",
    "                                                    random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "bc_scaler = StandardScaler()\n",
    "bc_scaler.fit(X_train)\n",
    "X_train_sc = bc_scaler.transform(X_train)\n",
    "X_test_sc = bc_scaler.transform(X_test)\n",
    "\n",
    "# Run the model\n",
    "bc_model = LogisticRegression(solver='lbfgs', max_iter=10000, \n",
    "                              random_state=42)\n",
    "bc_model.fit(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For this example, draw the ROC curve and calculate the AUC-ROC metric. Based on the results, do you think your model would be useful for identifying patients with breast cancer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your work here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "-----\n",
    "\n",
    "# Level Up: Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another more \"natural\" way of measuring the quality of a classifier is just to look at the loss function, which will often be the **log loss**. \n",
    "\n",
    "(In multiclass problems, we use **cross-entropy loss**.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "log_loss(y_test, logreg.predict_proba(X_test_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "While such loss values are difficult to interpret on their own, they are useful for comparing models. Models with lower loss values generate probability estimates that are closer to the true values, and thus are likely to perform better on many metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Log Loss by Hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Log loss is generally calculated as an average per data point, and is computed as follows:\n",
    "\n",
    "$L(y, \\hat{y}) = -\\frac{1}{N}\\sum^N_{i=1}[y_i\\ln(\\hat{y_i}) + (1-y_i)\\ln(1-\\hat{y_i})]$,\n",
    "\n",
    "where $y$ is the vector of true values and $\\hat{y}$ is the vector of probabilities that the point in question has a correct label of 1.\n",
    "\n",
    "- Suppose, for a given data point, that the correct prediction of the label is **0**. In that case, the contribution from that point to the sum in the loss function defined above will be $-\\ln(1-\\hat{y_i})$. So, the closer the prediction for that point is to 0, the closer the contribution to the sum will be to $-\\ln(1)=0$. But as the prediction gets closer to 1, the closer the contribution will be to $-\\ln(0)=\\infty$.\n",
    "\n",
    "- Suppose, on the other hand, that the correct prediction is **1**. In that case, the contribution from that point to the sum in the loss function defined above will be $-\\ln(\\hat{y_i})$. So, the closer the prediction for that point is to 1, the closer the contribution to the sum will be to $-\\ln(1)=0$. But as the prediction gets closer to 0, the closer the contribution will be to $-\\ln(0)=\\infty$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "compare = list(zip(y_test, logreg.predict_proba(X_test_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "compare[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "calc = [-(yi * np.log(yi_hat[1]) + (1 - yi) * np.log(yi_hat[0])) for (yi, yi_hat) in compare]\n",
    "calc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.mean(calc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
