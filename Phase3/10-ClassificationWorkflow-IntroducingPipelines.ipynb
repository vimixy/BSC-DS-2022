{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Workflow: Pipelines!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "- Formulate and implement an iterative modeling workflow\n",
    "- Recognize how pipelines streamline the preprocessing and modeling process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Pipeline?\n",
    "\n",
    "Pipelines can keep our code neat and clean all the way from gathering & cleaning our data, to creating models & fine-tuning them!\n",
    "\n",
    "**Advantages**: \n",
    "- Reduces complexity\n",
    "- Convenient \n",
    "- Flexible \n",
    "- Can help prevent mistakes (like data leakage between train and test set) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Today's Agenda: \n",
    "\n",
    "We'll introduce pipelines in the lens of simplifying the whole classification workflow, top to bottom!\n",
    "\n",
    "Our data: https://www.kaggle.com/c/cat-in-the-dat-ii\n",
    "\n",
    "The goal is to classify the `target` column. \n",
    "\n",
    "The competition's main metric is ROC-AUC score! We can explore other metrics, but we should be sure to use that to evaluate our models.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. Data Exploration\n",
    "2. Define and structure data preprocessing steps\n",
    "3. Run a `DummyClassifier` to create a model-less baseline, using a pipeline to combine the classifier with preprocessing steps\n",
    "4. Run a `LogisticRegression` and compare results to the model-less baseline\n",
    "\n",
    "If we have time, we'll keep iterating to improve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score, plot_roc_curve, plot_precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab, then explore data\n",
    "df = pd.read_csv(\"data/cat_in_the_dat2_train.csv\", index_col='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']\n",
    "\n",
    "# and train test split - to create our val holdout set!\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1,\n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Explore the **training** data, checking out both numeric and categorical features.\n",
    "\n",
    "We should run _at least_ one visualization to explore relationships among features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Let's outline our data processing strategy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss:\n",
    "\n",
    "Some questions we can ask ourselves:\n",
    "\n",
    "> How will we handle any null values? How will we handle any categorical features? What if our categorical features have 20+ unique values in each column? How will we scale our features?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's build a column transformer to define our data processing steps. Note that it's typically easiest to create list-like arrays of column names to match up with each processing step. Also - don't repeat columns! SKLearn's ColumnTransformer needs the lists of features it's processing to be mutually exclusive.\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Let's find out how hard our problem is, by creating a model-less baseline!\n",
    "\n",
    "If we use SKLearn's `DummyClassifier`, we can create our first full Pipeline!\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/modules/compose.html#pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Let's build an initial logistic regression model, using the same preprocessing steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate\n",
    "\n",
    "Let's either change something in our preprocessor, change something about our logistic regression model set up, or change the features we're using, then try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate\n",
    "\n",
    "How does our best model (so far) perform on our holdout val set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Check out Aur√©lien Geron's notebook of an [end-to-end ml project](https://github.com/ageron/handson-ml2/blob/master/02_end_to_end_machine_learning_project.ipynb) on his GitHub repo based around his book [_Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems (2nd ed)_](https://www.oreilly.com/library/view/hands-on-machine-learning/9781491962282/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up - What to do with too many options in categorical columns?\n",
    "\n",
    "\n",
    "New library you can install with MORE encoding techniques, beyond one-hot encoding! https://contrib.scikit-learn.org/category_encoders/\n",
    "\n",
    "- (these work within SKLearn pipelines, since they're written in the SKLearn style!)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
