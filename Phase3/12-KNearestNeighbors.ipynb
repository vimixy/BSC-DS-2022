{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# $k$-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Objectives\n",
    "\n",
    "- Describe the $k$-nearest neighbors algorithm\n",
    "- Identify multiple common distance metrics\n",
    "- Tune $k$ appropriately in response to models with high bias or variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who's Nearby?\n",
    "\n",
    "One strategy to make predictions on a new data point is to just look at what _similar_ data points are classified as."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### So: What should the grey point be?\n",
    "\n",
    "![example scenario](images/scenario.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## KNN has the following basic steps:\n",
    "\n",
    "![knn process](images/knn-process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The algorithm can be summarized as:**\n",
    "\n",
    "1. A positive integer `k` is specified\n",
    "2. We select the `k` entries in our training data which are closest to the new sample\n",
    "3. We find the most common classification of these entries (voting)\n",
    "4. This is the classification we give to the new sample\n",
    "\n",
    "**A few other features of KNN:**\n",
    "\n",
    "* KNN stores the entire training dataset which it uses as its representation\n",
    "* KNN does not learn any model parameters\n",
    "* KNN makes predictions 'just-in-time' by calculating the similarity between an input sample and each training instance\n",
    "\n",
    "**Note:** KNN performs better with a low number of features. The more features you have the more data you need. You increase the dimensions everytime you add another feature. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality.\n",
    "\n",
    "Resource on the Curse of Dimensionality for classification: https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Voting\n",
    "\n",
    "How to break ties:\n",
    "\n",
    "1. When doing a binary classification, often use a odd `k` to avoid ties.\n",
    "2. Multiple approaches for multi-class problems:\n",
    "    - Reduce the `k` by 1 to see who wins.\n",
    "    - Weight the votes based on the distance of the neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Distance Metrics\n",
    "\n",
    "When using KNN, we're assuming that _nearby_ points are _similar_ to one another. There are a few different wasy to determine how \"close\" data points are to one another:\n",
    "\n",
    "Distance metrics give us formulas to measure the distance between our variables:\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*FlMiuoENrq52tMV4S6LSZg.png)\n",
    "\n",
    "> Euclidean and Manhattan distance are typically best for continuous variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Implementing in Scikit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's try doing some basic classification on some data using the KNN algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same processed version of recent Austin Animal Shelter data\n",
    "# Goal is to predict whether an animal is adopted\n",
    "df = pd.read_csv(\"data/processed_shelter.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['adoption'], axis=1)\n",
    "y = df.adoption\n",
    "\n",
    "# Split into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42, test_size=.2)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Should We Scale?\n",
    "\n",
    "KNN IS a linear model - you **NEED** to scale your data before fitting a k-nearest neighbors model!\n",
    "\n",
    "KNN uses distances to determine neighbors, so it's comparing across features - it needs to have a consistent understanding of scale in order to do this appropriately!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_sc = scaler.transform(X_train)\n",
    "X_test_sc = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our KNN classifier - can use default k=5\n",
    "knn = None\n",
    "\n",
    "print(f\"Train Accuracy: {knn.score(X_train_sc, y_train)}\")\n",
    "print(f\"Test Accuracy: {knn.score(X_test_sc, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "test_preds = knn.predict(X_test_sc)\n",
    "\n",
    "print(f'Test Accuracy Score: {accuracy_score(y_test, test_preds)}')\n",
    "print('-------------------')\n",
    "print(f'Test Precision Score: {precision_score(y_test, test_preds)}')\n",
    "print('-------------------')\n",
    "print(f'Test Recall Score: {recall_score(y_test, test_preds)}')\n",
    "print('-------------------')\n",
    "print(f'Test F1_score: {f1_score(y_test, test_preds)}')\n",
    "print('-------------------')\n",
    "# Note the different input for ROC-AUC!\n",
    "print(f'Test ROC-AUC Score: {roc_auc_score(y_test, knn.predict_proba(X_test_sc)[:,1])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(knn, X_test_sc, y_test, display_labels=['not adopted', 'adopted']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Different $k$ Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A big factor in this algorithm is choosing $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# $k$ and the Bias-Variance Tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Let's slowly increase k and see what happens to our accuracy scores.\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "k_scores_train = {}\n",
    "k_scores_val = {}\n",
    "\n",
    "\n",
    "for k in range(1, 20, 2):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    accuracy_score_t = []\n",
    "    accuracy_score_v = []\n",
    "    for train_ind, val_ind in kf.split(X_train, y_train):\n",
    "        \n",
    "        X_t, y_t = X_train.iloc[train_ind], y_train.iloc[train_ind] \n",
    "        X_v, y_v = X_train.iloc[val_ind], y_train.iloc[val_ind]\n",
    "        mm = MinMaxScaler()\n",
    "        \n",
    "        X_t_ind = X_t.index\n",
    "        X_v_ind = X_v.index\n",
    "        \n",
    "        X_t = pd.DataFrame(mm.fit_transform(X_t))\n",
    "        X_t.index = X_t_ind\n",
    "        X_v = pd.DataFrame(mm.transform(X_v))\n",
    "        X_v.index = X_v_ind\n",
    "        \n",
    "        knn.fit(X_t, y_t)\n",
    "        \n",
    "        y_pred_t = knn.predict(X_t)\n",
    "        y_pred_v = knn.predict(X_v)\n",
    "        \n",
    "        accuracy_score_t.append(accuracy_score(y_t, y_pred_t))\n",
    "        accuracy_score_v.append(accuracy_score(y_v, y_pred_v))\n",
    "        \n",
    "        \n",
    "    k_scores_train[k] = np.mean(accuracy_score_t)\n",
    "    k_scores_val[k] = np.mean(accuracy_score_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_scores_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "k_scores_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.plot(list(k_scores_train.keys()), list(k_scores_train.values()),\n",
    "        color='red', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10, label='Train')\n",
    "ax.plot(list(k_scores_val.keys()), list(k_scores_val.values()),\n",
    "        color='green', linestyle='dashed', marker='o',  \n",
    "         markerfacecolor='blue', markersize=10, label='Val')\n",
    "ax.set_xlabel('k')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_xticks(ticks=list(k_scores_val.keys()))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss:\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## The Relation Between $k$ and Bias/Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Small $k$ values leads to overfitting, but larger $k$ values tend towards underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"images/K-NN_Neighborhood_Size_print.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> From [Machine Learning Flashcards](https://machinelearningflashcards.com/) by Chris Albon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "K-Nearest Neighbors is a **non-parametric**, **lazy** learning algorithm. \n",
    "\n",
    "What does this mean?\n",
    "\n",
    "- **Non-parametric models** make no underlying assumptions about the distribution of data\n",
    "- **Lazy learners** (or **instance-based** learning methods) simply store the training examples and postpone the generalization (building a model) until a new instance must be classified or prediction made\n",
    "    - In other words, no training is necessary! This makes training super fast but testing is slower and costly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Lazy learning (no training phase)\n",
    "- Simple algorithm to understand and implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Computationally expensive\n",
    "    - It doesn't learn anything when the model is `fit` - it just stores all the training data points in memory\n",
    "    - Thus: high memory requirement, prediction stage can be slow\n",
    "- Not robust - doesn't generalize well\n",
    "- Soft boundaries are troublesome\n",
    "- \"Curse of Dimensionality\"\n",
    "    - Sensitive to irrelevant features and the scale of data\n",
    "    - Thus: doesn't work well with a lot of input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "# Additional Resources\n",
    "\n",
    "- [Nearest Neighbors](http://scikit-learn.org/stable/modules/neighbors.html) (user guide), [KNeighborsClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) (class documentation)\n",
    "\n",
    "- [Videos from An Introduction to Statistical Learning](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n",
    "    - Classification Problems and K-Nearest Neighbors (Chapter 2)\n",
    "    - Introduction to Classification (Chapter 4)\n",
    "    - Logistic Regression and Maximum Likelihood (Chapter 4)\n",
    "    \n",
    "- [Curse of Dimensionality](https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)\n",
    "\n",
    "### More Resources on Scaling:\n",
    "\n",
    "https://sebastianraschka.com/Articles/2014_about_feature_scaling.html   \n",
    "http://datareality.blogspot.com/2016/11/scaling-normalizing-standardizing-which.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Visualize the Decision Boundaries\n",
    "\n",
    "One way to think about KNN is that there are decision boundaries, areas in which we know exactly what the prediction will be for some given `k`, since we can calculate how many points of each class are nearby. SKLearn and matplotlib can help us visualize this!\n",
    "\n",
    "Reference: https://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's read in the Chemical Manufacturing Process data\n",
    "chem_df = pd.read_csv(\"data/ChemicalManufacturingProcess.csv\", index_col=0)\n",
    "chem_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cols = ['BiologicalMaterial01', 'BiologicalMaterial02']\n",
    "X = chem_df[used_cols]\n",
    "y = (chem_df['Yield'] >= 41).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n",
    "\n",
    "# Can adjust our number of neighbors to explore\n",
    "n_neighbors = 5\n",
    "h = .02  # step size in the mesh\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf = KNeighborsClassifier(n_neighbors, weights=weights)\n",
    "    clf.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X.values[:, 0].min() - 1, X.values[:, 0].max() + 1\n",
    "    y_min, y_max = X.values[:, 1].min() - 1, X.values[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X.values[:, 0], X.values[:, 1], c=y, cmap=cmap_bold)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"Binary Classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Level Up: Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> The \"closeness\" of data points → proxy for similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](img/distances.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Minkowski Distance**:\n",
    "\n",
    "$$dist(A,B) = (\\sum_{k=1}^{N} |a_k - b_k|^c)^\\frac{1}{c} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Special cases of Minkowski distance are:\n",
    "\n",
    "- Manhattan: $dist(A,B) = \\sum_{k=1}^{N} |a_k - b_k|$\n",
    "\n",
    "\n",
    "- Euclidean: $dist(A,B) = \\sqrt{ \\sum_{k=1}^{N} (a_k - b_k)^2 }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are quite a few different distance metrics built-in for Scikit-learn: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "TOC",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
