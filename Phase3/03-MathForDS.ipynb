{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math for Data Science\n",
    "\n",
    "![math is coming meme](images/Brace-yourself-Math.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "np.random.seed(42)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Linear Algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Scalars, Vectors, Matrices, Tensors: It's all about the dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![different_tensors.png](images/different_tensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tensors** are a general entity/way of understanding space and numbers which have *ranks* (or *orders*)\n",
    "\n",
    "- **Scalar**: a 0-rank tensor\n",
    "- **Vector**: a 1st-rank tensor\n",
    "- **Matrix**: a 2nd-rank tensor\n",
    "\n",
    "You can have higher-rank tensors, for example a 3rd-rank tensor that captures an image across three color channels (RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectors\n",
    "\n",
    "In the context of linear algebra, a single number is a 0-dimensional entity called a **scalar**. But it is often useful to have data in the form of a 1-dimensional object called a **vector**, which can be thought of as a list of scalars. Think here of a `pandas` Series. And in addition to the values that compose the vector, we can characterize the vector as a whole as having a **magnitude** (size) and a **direction** (angle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(3,4, s=30, color='black')\n",
    "ax.arrow(0, 0, 3, 4, color='b', length_includes_head=True, head_width=.2, overhang=.9)\n",
    "ax.vlines(1, ymin=0, ymax=4/3)\n",
    "\n",
    "ax.annotate('$\\phi$', xy=(1.1, 0.5))\n",
    "ax.annotate('|v| = 5', xy=(0.9, 2.3))\n",
    "ax.set_xlim(right=5)\n",
    "ax.set_ylim(top=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Calculations\n",
    "\n",
    "There are many times when we need to measure distances. For example, many modeling algorithms rely on a notion of **similarity** between data points.\n",
    "\n",
    "In linear algebra, the **norm** behaves like a calculation of the distance from the origin. In other words, it captures the length or magnitude of the vector. \n",
    "\n",
    "But... how are we considering distance? In the above graph, we calculated the magnitude of the vector as $5$ - that's the _Euclidean distance_ between the origin ($[0,0]$) and our vector ($[3,4]$).\n",
    "\n",
    "magnitude = $\\sqrt{(3 - 0)^2 + (4 - 0)^2} = 5$\n",
    "\n",
    "The Euclidean distance is captured by the **L2** norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# By hand...\n",
    "np.sqrt((3 - 0)**2 + (4 - 0)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using numpy\n",
    "# Create our vector, a 1-dimensional tensor\n",
    "vector = np.array([3,4])\n",
    "\n",
    "# Now calculate that distance - using L2 aka ord=2\n",
    "np.linalg.norm(vector, ord=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's also the L1 norm, calculating the Manhattan distance\n",
    "\n",
    "# Visualize the Manhattan distance\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.scatter(3,4, s=30, color='black')\n",
    "ax.vlines(3, ymin=0, ymax=4, color='blue')\n",
    "ax.hlines(0, xmin=0, xmax=3, color='blue')\n",
    "\n",
    "ax.set_xlim(right=5)\n",
    "ax.set_ylim(top=6);\n",
    "plt.show()\n",
    "\n",
    "# Now calculate that distance - using L1 aka ord=1\n",
    "print(f\"L1 Norm: {np.linalg.norm(vector, ord=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point? Linear algebra here gives us a way to deal with space, and calculate direction - and we can use it to calculate distances between points!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have been working all along with arrays and data frames that have a tabular structure of rows and columns. Such a 2-dimensional structure of numerical elements is known in linear algebra as a **matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load up a dataset from seaborn real quick...\n",
    "gems = sns.load_dataset('diamonds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it out\n",
    "gems.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a matrix under the hood!\n",
    "gems.head().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices can be added and multiplied, and there are other distinctive operations on matrices that are often useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Matrix Addition</b>: Click for details</summary>\n",
    "$\\begin{bmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\begin{bmatrix}\n",
    "b_{11} & b_{12} \\\\\n",
    "b_{21} & b_{22}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{11} + b_{11} & a_{12} + b_{12} \\\\\n",
    "a_{21} + b_{21} & a_{22} + b_{22}\n",
    "\\end{bmatrix}\n",
    "$\n",
    "    </details>\n",
    "    \n",
    "Note: Typically happens element-wise, but if the dimensions don't add up, numpy will _broadcast_ the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix1 = np.random.randint(low=1, high=11, size=(2, 2))\n",
    "my_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix2 = np.random.randint(low=1, high=11, size=(2, 2))\n",
    "my_matrix2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix1 + my_matrix2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><b>Matrix Multiplication</b>: Click for details</summary>\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "$$\n",
    "    </details>\n",
    "    \n",
    "Specifically, the dot product! Likely the most common operation when we think of \"multiplying\" matrices.\n",
    "\n",
    "Note that the first matrix's length needs to match the second matrix's width (they need to share one opposing dimension)\n",
    "\n",
    "(m-by-**n**) DOT (**n**-by-p) ==> (m-by-p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_matrix1.dot(my_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving a System of Equations with Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In elementary algebra, we start by solving one equation for one unknown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image from mathelp.org](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSnrLW6J0FYge6zWDKqRrAtWx4Jf0HkhMiwHQ&usqp=CAU) image source: mathelp.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear algebra gives us the tools to solve many equations simultaneously. Suppose we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\begin{align}\n",
    " x_1 - 2x_2 + 3x_3 &= 9 \\\\\n",
    " 2x_1 - 5x_2 + 10x_3 &= 4 \\\\\n",
    " 6x_3 &= 0 \n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write these equations as a single matrix equation:\n",
    "\n",
    "$$ \n",
    "\\begin{bmatrix} \n",
    "    1 & -2 & 3 \\\\\n",
    "    2 & -5 & 10 \\\\\n",
    "    0 & 0 & 6\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} \n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    x_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix} \n",
    "    9 \\\\\n",
    "    4 \\\\\n",
    "    0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Or: $A\\vec{x} = \\vec{b}$, where\n",
    "\n",
    "- $A = \\begin{bmatrix} \n",
    "    1 & -2 & 3 \\\\\n",
    "    2 & -5 & 10 \\\\\n",
    "    0 & 0 & 6\n",
    "\\end{bmatrix}$\n",
    "\n",
    "- $\\vec{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ x_3 \\end{bmatrix}$\n",
    "\n",
    "- $\\vec{b} = \\begin{bmatrix} 9 \\\\ 4 \\\\ 0 \\end{bmatrix}$\n",
    "\n",
    "So now we're solving for a *vector* of unknowns. To solve $A\\vec{x} = \\vec{b}$ for $x$, we multiply both sides of the equation by **$A^{-1}$, the inverse of $A$**:\n",
    "\n",
    "$A^{-1}A\\vec{x} = A^{-1}\\vec{b}$\n",
    "\n",
    "which, after $A$ and $A^{-1}$ cancel out, becomes:\n",
    "\n",
    "$\\vec{x} = A^{-1}\\vec{b}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([\n",
    "    [1, -2,  3],\n",
    "    [2, -5, 10],\n",
    "    [0,  0,  6]\n",
    "])\n",
    "\n",
    "b = np.array([9, 4, 0]).reshape(3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the inverse of A\n",
    "inv_A = np.linalg.inv(A)\n",
    "\n",
    "# Then using the dot product to multiply inv_A and b\n",
    "# Because matrix multiplication is complicated... but dot products are great!\n",
    "inv_A.dot(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use the .solve() method\n",
    "np.linalg.solve(A, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I said that $A$ and $A^{-1}$ cancel out. Technically, multiplying a matrix by its inverse produces $I$, the **identity matrix**, a square matrix with 1's down the main diagonal and 0's everywhere else. \n",
    "\n",
    "Inverse and identity matrices have important properties:\n",
    "\n",
    "- $IA = A$\n",
    "- $AI = A$\n",
    "- $AA^{-1} = I$\n",
    "- $A^{-1}A = I$\n",
    "- $I\\vec{x} = \\vec{x}$\n",
    "\n",
    "(but yes in practice they cancel out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Taste of the Linear Algebra to Come\n",
    "\n",
    "AKA why do data scientists care about linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues, Singular Values, Principal Components\n",
    "\n",
    "It is often useful to express a matrix as a **product** of other matrices. Sometimes the gain is only in computational efficiency, but there are also certain factorizations or **decompositions** that are useful in other ways.\n",
    "\n",
    "An **eigendecomposition** reduces a matrix to a collection of vectors that capture the *linear* action of the matrix. Selecting the vectors that produce the largest such linear transformations is the idea behind **principal component analysis**, which is useful for reducing high-dimensional datasets to lower-dimensional problems.\n",
    "\n",
    "Eigendecompositions are possibly only for square matrices; a **singular value decomposition** is a more fundamental matrix factorization that can be applied to any matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation Systems\n",
    "\n",
    "How do recommendation engines work?\n",
    "\n",
    "Imagine representing your interests (film genres, book subjects, music styles) as a **vector**: larger numbers represent larger preferences. Now do this for multiple people. Now we can think of comparing these vectors directly or against some target such as whether a given product/service was used/bought/watched."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "When our data is **unlabeled** we have a problem in **unsupervised learning**. One major strategy for this type of problem is to impose a *similarity* metric on our data points. Similarity between data points is measured as some function of the **(vector) distance** between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "\n",
    "One similarity metric for vectors is **cosine similarity**, which computes the *cosine of the angle between them*. Note that this is always well-defined for non-zero vectors since any two vectors determine a plane (in which the angle can be measured)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Images\n",
    "\n",
    "We saw already above the idea of representing a digital image as a **tensor** of values that encode facts about each pixel in the digitization. **Neural networks** are good for working with tensors of high dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Resources & References\n",
    "\n",
    "- [10 Powerful Applications of Linear Algebra in Data Science](https://www.analyticsvidhya.com/blog/2019/07/10-applications-linear-algebra-data-science/) (if you're still wondering why you need to care!)\n",
    "- [Introduction to Applied Linear Algebra: Norms & Distances](https://towardsdatascience.com/introduction-to-applied-linear-algebra-norms-distances-2451e6325925)\n",
    "- [Gentle Introduction to Vector Norms in Machine Learning](https://machinelearningmastery.com/vector-norms-machine-learning/)\n",
    "- [Scalars, Vectors, Matrices and Tensors - Linear Algebra for Deep Learning (Part 1)](https://www.quantstart.com/articles/scalars-vectors-matrices-and-tensors-linear-algebra-for-deep-learning-part-1/) from QuantStart - a thorough detailing of linear algebra specifically through the lens of data science. This first post is discussing the difference between different-rank tensors, but they have several other posts in this series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## What Has Calculus Ever Done For You?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<img src=\"https://c.tenor.com/nr56QSWEkzwAAAAd/calculus-cat-cat-math.gif\" alt=\"cat learning calculus gif from tenor\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We have already had occasion to use calculus in a few places. Calculus shows us:\n",
    "\n",
    "- that the mean of a group of numbers is the number $n$ that minimizes the sum of squared differences $\\Sigma(p-n)^2$ for each number $p$ in the group;\n",
    "- that the median of a group of numbers is the number $m$ that minimizes the sum of absolute differences $\\Sigma|p-m|$ for each number $p$ in the group;\n",
    "- a way to find the coefficients for a linear regression optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The two main tools of calculus are **differentiation** and **integration**. For functions of one dimension:\n",
    "\n",
    "- Differentiation gives us the *slope* of the function at any point.\n",
    "- Integration gives us the *area under the curve* of the function between any two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![calc](https://t3.ftcdn.net/jpg/04/02/03/54/240_F_402035410_ihv2bsAJAWRLG1BiFGtpiCUEMJXVxtbW.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Surprisingly, these two operations turn out to be inverses of one another, in the sense that the derivative of the integral of a given function takes us back to the initial function. This is known as the First Fundamental Theorem of Calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To find the slope of a function *at a point*, we imagine calculating the slope of the function between two points, and then gradually bringing those two points together. Consider the slope of the function $y=x^2$ at the point $x=100$.\n",
    "\n",
    "We'll calculate the slope of the parabola between $x_1=100$ and $x_2=1000$, and then slowly move $x_2$ close to $x_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = np.logspace(3, 1, 21)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, X**2, c='black')\n",
    "for x_ in X[:11]:\n",
    "    ax.plot([100, x_], [10000, x_**2], 'r-.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Integration is how we calculate the area under a curve. \n",
    "\n",
    "How do you calculate the area of a shape with a curvy side? Imagine approximating the shape with rectangles, and then imagine making those rectangles narrower and narrower.\n",
    "\n",
    "Again, let's work with the parabola $y=x^2$ between $x=100$ and $x=1000$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This will show how we imagine ever narrower rectangles\n",
    "# under the curve to approximate the area underneath it.\n",
    "\n",
    "spacing = np.arange(3, 13)\n",
    "\n",
    "X = [np.linspace(100, 1000, step) for step in spacing]\n",
    "X_curve = np.linspace(100, 1000, 10000)\n",
    "\n",
    "fig, ax = plt.subplots(10, figsize=(6, 20))\n",
    "\n",
    "for num in spacing:\n",
    "    ax[num-3].plot(X_curve, X_curve**2, color='black')\n",
    "    for j in range(1, len(X[num-3])-1):\n",
    "        ax[num-3].hlines(X[num-3][j]**2, X[num-3][j], X[num-3][j+1])\n",
    "        ax[num-3].vlines(X[num-3][j], 0, X[num-3][j]**2)\n",
    "    ax[num-3].set_xlabel(f'Area = {900/(num-1) * sum(X[num-3][1:-1]**2)}\\n\\\n",
    "    For a=100, b=1000, $\\int^b_ax^2=333000000$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This will show the area of the rectangles as the number\n",
    "# of rectangles increases.\n",
    "\n",
    "spacing_longer = np.arange(3, 100)\n",
    "X_longer = [np.linspace(100, 1000, step) for step in spacing_longer]\n",
    "\n",
    "areas = [900 / (num-1) * sum(X_longer[num-3][1:-1]**2) for num in spacing_longer]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.hlines(333000000, 3, 99, label='333000000', color='r')\n",
    "ax.plot(spacing_longer, areas, label='approximation')\n",
    "ax.set_title('Area as a function of number of rectangles')\n",
    "ax.set_xlabel('Number of rectangles')\n",
    "ax.set_ylabel('Area')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Machine Learning\n",
    "\n",
    "This is a good time to think for a second about what it means to build models. Ultimately we're saying that, given some number of inputs (x, or features in our datasets) we can predict the outcome (y, or our target variable).\n",
    "\n",
    "When we first started with simple linear regression, we were trying to find a **line of best fit** (and we'll get to that \"best fit\" part in a second) which captures some way of taking in a single variable (x) and transforming it (by maybe multiplying by a slope and maybe adding some constant) to find a continuous output (y).\n",
    "\n",
    "> Example: if we believe the miles per gallon rate of a car is just a function of its horsepower, we would try to find the rate at which the horsepower then changes the MPG, plus some constant if the MPG isn't exactly zero when the horsepower is zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the data from 'auto-mpg.csv'\n",
    "mpg_df = pd.read_csv(\"data/auto-mpg.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the data\n",
    "mpg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try this out ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting the actual relationship between horsepower and mpg\n",
    "sns.scatterplot(x='horsepower', y='mpg', data=mpg_df)\n",
    "\n",
    "plt.xticks(ticks=range(0, 250, 50))\n",
    "plt.yticks(ticks=range(0, 55, 5))\n",
    "\n",
    "plt.title('Relationship Between Horsepower and MPG')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, with machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a linear regression model using just the horsepower column\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = mpg_df[['horsepower']]\n",
    "y = mpg_df['mpg']\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "m = model.coef_[0] # Find the slope - coefficient for the single variable in X\n",
    "b = model.intercept_ # Find the intercept\n",
    "\n",
    "print(f'Slope: {m}')\n",
    "print(f'Intercept: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the prediction for a row by hand\n",
    "# y = mx + b\n",
    "y_pred_row0 = m * mpg_df['horsepower'][0] + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same as using sklearn\n",
    "model.predict(mpg_df['horsepower'][0].reshape(1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Actual: {mpg_df['mpg'][0]} - Predicted: {y_pred_row0} = {mpg_df['mpg'][0] - y_pred_row0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot the line over the actual values\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plotting the actual relationship between horsepower and mpg\n",
    "sns.scatterplot(x='horsepower', y='mpg', data=mpg_df)\n",
    "\n",
    "# Now plotting the line of best fit found by our model\n",
    "x_vals = range(0, 250)\n",
    "y_vals = b + m * x_vals\n",
    "# Would be the same as model.predict(np.array(x_vals).reshape(-1,1))\n",
    "plt.plot(x_vals, y_vals, '--', color='orange')\n",
    "\n",
    "plt.title('Relationship Between Horsepower and MPG')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph shows the approximate best fit line for the relationship between `horsepower` and `mpg` in our data, which we found through sklearn's function. But what if we wanted to test how it changes if we change that slope?\n",
    "\n",
    "As you can imagine, as we test out different slopes, keeping the y-intercept constant, we can see how that affects our error - our Residual Sum of Squares (RSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember Our Cost Function\n",
    "\n",
    "> A cost function is a function that calculates the error of our models predictions vs ground truth.\n",
    ">\n",
    "> \"Cost function\" = \"Loss function\" = \"Error\"\n",
    "    \n",
    "    \n",
    "#### Residual Sum of Squares\n",
    "\n",
    "Ordinary least squares regression tries to minimize the RSS:\n",
    "\n",
    "$ \\large RSS = \\sum_{i=1}^n(actual - expected)^2 = \\sum_{i=1}^n(y_i - \\hat{y})^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RSS(x_values, y_values, m, b):\n",
    "    y_pred = (b + m*x_values)\n",
    "    return np.sum(np.square(y_pred - y_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mpg_df['horsepower'] # Grabbing x as a series this time\n",
    "y = mpg_df['mpg'] # And our target\n",
    "\n",
    "results = {}\n",
    "# Testing 20 slope options between -0.5 and 0.5\n",
    "for slope in np.linspace(-0.5, 0.5, 20):\n",
    "    results[slope] = RSS(x, y, slope, 39.93) # remember, holding b constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in results.items():\n",
    "    print(f\"Slope: {k}: {v:,.2f}\") # using :,.2f to round and add commas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below graph zooms in to show how the model chose that exact slope for the line of best fit, by showing the residual sum of squares (RSS) as you change the slope of that line.\n",
    "\n",
    "![Slope-RSS relationship image](images/slope-rss-relationship.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above curve is just for ONE coefficient, the slope, while holding the y-intercept constant. Already with simple linear regression we have to start using gradients, from partial derivatives, in order to optimize both parameters at the same time.\n",
    "\n",
    "If we imagine all the different ways we can adjust **both** parameters and measure how well the model performs with the loss or **cost function**, we can plot this as a surface in this multidimensional plane:\n",
    "\n",
    "<img alt=\"gradient descent image from Section 23 - Gradient Descent: Step Sizes\" src=\"images/gradientdescent.png\" width=400>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then imagine how much more complicated this cost curve would become as we realize that more than one variable could have an impact on our target, and move beyond simple linear regression!\n",
    "\n",
    "> Back to the example: If horsepower alone cannot be used to predict the miles per gallon rate of a car, maybe the horsepower plus the number of cylinders plus the weight of the car actually can better predict the MPG. Each of those three separate variables will have their own way of changing the MPG, that rate of change, and in the end you might have some constant term too\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "![gradient descent in 3d gif from Andrew Ng](https://thumbs.gfycat.com/AngryInconsequentialDiplodocus-size_restricted.gif)\n",
    "\n",
    "**Gradient descent** is an optimization procedure that uses the _gradient_ (a generalized notion of a derivative) of the cost function.\n",
    "\n",
    "The gradient helps us recognize how to go \"downhill\" towards the lowest point, which would minimize the cost function.\n",
    "\n",
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. A large value of the derivative with respect to a particular variable means that the gradient will have a large component in the corresponding direction. Therefore, **the gradient will point in the direction of steepest increase**.\n",
    "\n",
    "If we want to improve our guess at the minimum of our loss function, we'll move in the **opposite direction** of the gradient away from our last guess. Hence we are using the *gradient* of our loss function to *descend* to the minimum value of the relevant loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Summary of Gradient Descent:\n",
    "\n",
    "- Make a guess at where the function attains its minimum value\n",
    "- Calculate the gradient/derivative at that point\n",
    "- Use that value to decide how to make your next guess!\n",
    "\n",
    "Repeat until we get the derivative as close as we like to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Size\n",
    "\n",
    "As we calculate the gradient (or, rather, as our model does) we make changes, see what happens, and make more changes - all in an educated, math-informed way.\n",
    "\n",
    "> Let's call each of these changes a **step**, and the size of the change our **step size**. \n",
    "\n",
    "If our steps are _too big_, we risk skipping over the minimum value (optimal parameters).\n",
    "\n",
    "BUT if our steps are _too small_, it might take us too long to reach the minimum value.\n",
    "\n",
    "Here's an elegant solution: Make the size of your step **proportional to the value of the derivative at the point where you currently are in parameter space**! If we're very far from the minimum, then our values will be large, and so we therefore can safely take a large step; if we're close to the minimum, then our values will be small, and so we should therefore take a smaller step.\n",
    "\n",
    "I said the size of the step is proportional to the value of the derivative. The constant of proportionality is often called the **\"learning rate\"**. \n",
    "\n",
    "1. A small learning rate requires many updates before reaching the minimum \n",
    "2. The optimal learning rate quickly converges to the minimum point \n",
    "3. A learning rate that is too large leads to divergent behavior: you may bounce around the minimum!  \n",
    "\n",
    "![learning_rate](https://www.jeremyjordan.me/content/images/2018/02/Screen-Shot-2018-02-24-at-11.47.09-AM.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus Resources and References\n",
    "\n",
    "\n",
    "- The same guy who runs the 3 Blue 1 Brown account also does some videos for Khan Academy, including the [Partial Derivatives videos from the Multivariable Calculus course](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives)\n",
    "\n",
    "#### Gradient Descent:\n",
    "\n",
    "- Andrew Ng was a famous name in machine learning before he created the ultra-popular [Machine Learning Coursera course](https://www.coursera.org/learn/machine-learning), which has a great explanation of gradient descent\n",
    "\n",
    "    - [This blog post by Chris McCormick](https://mccormickml.com/2014/03/04/gradient-descent-derivation/) breaks out some of the explanation from the course, specifically trying to derive the math\n",
    "    \n",
    "- [Machine Learning Mastery has a tutorial on Gradient Descent for Machine Learning](https://machinelearningmastery.com/gradient-descent-for-machine-learning/), walks through gradient descent with surprisingly little math\n",
    "\n",
    "- This page helps to explain the dangers of learning rates that are too large and too small: https://www.jeremyjordan.me/nn-learning-rate/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "# Level Up Section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Tips for Gradient Descent\n",
    "\n",
    "A few of the tips mentioned in [Machine Learning Mastery's tutorial on Gradient Descent for Machine Learning](https://machinelearningmastery.com/gradient-descent-for-machine-learning/)\n",
    "\n",
    "- **Plot Cost versus Time:** Collect and plot the cost values calculated by the algorithm each iteration, when we get to models that give you their iterations. The expectation for a well performing gradient descent run is a decrease in cost each iteration. If it does not decrease, try reducing your learning rate.\n",
    "- **Learning Rate:** The learning rate value is a small real value such as 0.1, 0.001 or 0.0001. Try different values for your problem and see which works best.\n",
    "- **Rescale Inputs:** The algorithm will reach the minimum cost faster if the shape of the cost function is not skewed and distorted. You can achieved this by rescaling all of the input variables (X) to the same range, such as [0, 1] or [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: Matrix Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many transformations of *products* of matrices can be expressed in terms of the transformation applied to the factors *in reverse order*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(AB)^T = B^TA^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(low=1, high=11, size=(10, 2))\n",
    "B = np.random.randint(low=1, high=11, size=(2, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(A.dot(B)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B.T.dot(A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$(AB)^{-1} = B^{-1}A^{-1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(low=1, high=11, size=(3, 3))\n",
    "B = np.random.randint(low=1, high=11, size=(3, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(A.dot(B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.linalg.inv(B).dot(np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Level Up: The Determinant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinant\n",
    "\n",
    "The **determinant** of a square matrix $M$, $|M|$, represents the area (or, in higher dimensions, the volume) of the parallelogram (parallelepiped) formed by the rows or columns of $M$. And it is also related to the inverse of $M$.\n",
    "\n",
    "For a 2x2 matrix $\\begin{bmatrix} a & b \\\\ c & d\\end{bmatrix}$, the determinant is equal to $ad - bc$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_matrix1 = np.random.randint(low=1, high=11, size=(2, 2))\n",
    "\n",
    "my_matrix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.det(my_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = my_matrix1[0][0]\n",
    "d = my_matrix1[1][1]\n",
    "b = my_matrix1[0][1]\n",
    "c = my_matrix1[1][0]\n",
    "\n",
    "a*d - b*c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Leel Up: The Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$\\large\\frac{d}{dx}[f(g(x))] = f'(g(x))g'(x)$\n",
    "\n",
    "That is: The derivative of a *composition* of functions is: the derivative of the first applied to the second, multiplied by the derivative of the second.\n",
    "\n",
    "So if we know e.g. that $\\frac{d}{dx}[e^x] = e^x$ and $\\frac{d}{dx}[x^2] = 2x$, then we can use the Chain Rule to calculate $\\frac{d}{dx}[e^{x^2}]$. We set $f(x) = e^x$ and $g(x) = x^2$, so the derivative must be:\n",
    "\n",
    "$\\large\\frac{d}{dx}[e^{x^2}] = (e^{x^2})(2x) = 2xe^{x^2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Exercise:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calculate the derivatives for the following compositions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. $\\frac{d}{dx}[sin(4x)]$\n",
    "\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    $f(x) = sin(x)$ <br/>\n",
    "    $g(x) = 4x$ <br/>\n",
    "    So the derivative will be: $cos(4x)*4 = 4cos(4x)$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. $\\frac{d}{dx}[e^{sin(x)}]$\n",
    "\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    $f(x) = e^x$ <br/>\n",
    "    $g(x) = sin(x)$ <br/>\n",
    "    So the derivative will be: $e^{sin(x)}*cos(x) = cos(x)e^{sin(x)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Level Up: Partial Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Partial differentiation is required for functions of multiple variables. If e.g. I have some function $h = h(a, b)$, then I can consider how $h$ changes with respect to $a$ (while keeping $b$ constant)––that's $\\frac{\\partial h}{\\partial a}$, and I can consider how $h$ changes with respect to $b$ (while keeping $a$ constant)––that's $\\frac{\\partial h}{\\partial b}$. And so the rule is simple enough: If I'm differentiating my function with respect to some variable, I'll **treat all other variables as constants**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Consider the following function:\n",
    "\n",
    "$\\large\\xi(x, y, z) = x^2y^5z^3 - ze^{cos(xy)} + (yz)^3$;\n",
    "\n",
    "for some parameters $x$, $y$, and $z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are the partial derivatives of this function?\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial x} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $2xy^5z^3 + yze^{cos(xy)}sin(xy)$\n",
    "    </details>\n",
    "<br/>\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial y} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $5x^2y^4z^3 + xze^{cos(xy)}sin(xy) + 3y^2z^3$\n",
    "    </details>\n",
    "<br/>\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial z} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $3x^2y^5z^2 - e^{cos(xy)} + 3y^3z^2$\n",
    "    </details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
